<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="The activation module."><title>burn::tensor::activation - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2"href="../../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../../static.files/rustdoc-e56847b5.css"><meta name="rustdoc-vars" data-root-path="../../../" data-static-root-path="../../../static.files/" data-current-crate="burn" data-themes="" data-resource-suffix="" data-rustdoc-version="1.91.1 (ed61e7d7e 2025-11-07)" data-channel="1.91.1" data-search-js="search-e256b49e.js" data-stringdex-js="stringdex-c3e638e9.js" data-settings-js="settings-c38705f0.js" ><script src="../../../static.files/storage-e2aeef58.js"></script><script defer src="../sidebar-items.js"></script><script defer src="../../../static.files/main-6dc2a7f3.js"></script><noscript><link rel="stylesheet" href="../../../static.files/noscript-263c88ec.css"></noscript><link rel="alternate icon" type="image/png" href="../../../static.files/favicon-32x32-eab170b8.png"><link rel="icon" type="image/svg+xml" href="../../../static.files/favicon-044be391.svg"></head><body class="rustdoc mod"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><rustdoc-topbar><h2><a href="#">Module activation</a></h2></rustdoc-topbar><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../../../burn/index.html">burn</a><span class="version">0.16.1</span></h2></div><div class="sidebar-elems"><section id="rustdoc-toc"><h2 class="location"><a href="#">Module activation</a></h2><h3><a href="#functions">Module Items</a></h3><ul class="block"><li><a href="#functions" title="Functions">Functions</a></li></ul></section><div id="rustdoc-modnav"><h2><a href="../index.html">In burn::<wbr>tensor</a></h2></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../../index.html">burn</a>::<wbr><a href="../index.html">tensor</a></div><h1>Module <span>activation</span>&nbsp;<button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../../src/burn_tensor/tensor/mod.rs.html#18">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>The activation module.</p>
</div></details><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.gelu.html" title="fn burn::tensor::activation::gelu">gelu</a></dt><dd>Applies the Gaussian Error Linear Units function as described in the paper <a href="https://arxiv.org/pdf/1606.08415v3.pdf">Gaussian Error Linear Units (GELUs)</a>.</dd><dt><a class="fn" href="fn.hard_sigmoid.html" title="fn burn::tensor::activation::hard_sigmoid">hard_<wbr>sigmoid</a></dt><dd>Applies the hard sigmoid function</dd><dt><a class="fn" href="fn.leaky_relu.html" title="fn burn::tensor::activation::leaky_relu">leaky_<wbr>relu</a></dt><dd>Applies the leaky rectified linear unit function.</dd><dt><a class="fn" href="fn.log_sigmoid.html" title="fn burn::tensor::activation::log_sigmoid">log_<wbr>sigmoid</a></dt><dd>Applies the log sigmoid function.</dd><dt><a class="fn" href="fn.log_softmax.html" title="fn burn::tensor::activation::log_softmax">log_<wbr>softmax</a></dt><dd>Applies the log softmax function on the input tensor along the given dimension.</dd><dt><a class="fn" href="fn.mish.html" title="fn burn::tensor::activation::mish">mish</a></dt><dd>Applies the Mish function as described in the paper in <a href="https://arxiv.org/abs/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a>.</dd><dt><a class="fn" href="fn.prelu.html" title="fn burn::tensor::activation::prelu">prelu</a></dt><dd>Applies Parametric ReLu activation function as described in the paper <a href="https://arxiv.org/pdf/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>.
<code> PReLu(x) = max(0,x) + \alpha * min(0,x)</code>
tensor is assumed to be of shape [batch_size, channels, …]
alpha is assumed to be of shape [channels] or [1]</dd><dt><a class="fn" href="fn.quiet_softmax.html" title="fn burn::tensor::activation::quiet_softmax">quiet_<wbr>softmax</a></dt><dd>Applies the “quiet softmax” function on the input tensor along the given dimension.
This function is similar to the softmax function, but it allows for “no selection”, e.g.,
all outputs can tend to zero.</dd><dt><a class="fn" href="fn.relu.html" title="fn burn::tensor::activation::relu">relu</a></dt><dd>Applies the rectified linear unit function as described in the paper <a href="https://arxiv.org/pdf/1803.08375">Deep Learning using
Rectified Linear Units (ReLU)</a>.</dd><dt><a class="fn" href="fn.sigmoid.html" title="fn burn::tensor::activation::sigmoid">sigmoid</a></dt><dd>Applies the sigmoid function.</dd><dt><a class="fn" href="fn.silu.html" title="fn burn::tensor::activation::silu">silu</a></dt><dd>Applies the silu function</dd><dt><a class="fn" href="fn.softmax.html" title="fn burn::tensor::activation::softmax">softmax</a></dt><dd>Applies the softmax function on the input tensor along the given dimension.</dd><dt><a class="fn" href="fn.softmin.html" title="fn burn::tensor::activation::softmin">softmin</a></dt><dd>Applies the softmin function on the input tensor along the given dimension.</dd><dt><a class="fn" href="fn.softplus.html" title="fn burn::tensor::activation::softplus">softplus</a></dt><dd>Applies the softplus function</dd><dt><a class="fn" href="fn.tanh.html" title="fn burn::tensor::activation::tanh">tanh</a></dt><dd>Applies the tanh function</dd></dl></section></div></main></body></html>
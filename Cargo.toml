[package]
name = "gllm-kernels"
version = "0.2.0"
edition = "2021"
authors = ["gllm contributors"]
license = "Apache-2.0"
description = "Low-level attention kernels with runtime backend selection (CUDA/ROCm/Metal/WGPU/CPU)"
repository = "https://github.com/putao520/gllm-kernels"
homepage = "https://github.com/putao520/gllm-kernels"
keywords = ["attention", "kernels", "burn", "gpu", "tensor"]
categories = ["algorithms", "science"]

[lib]
name = "gllm_kernels"
path = "src/lib.rs"

[features]
# 默认：全后端支持，运行时动态选择
default = ["full"]

# 完整版：所有后端 + 所有 kernel（Fat Binary，全部预编译嵌入）
full = ["all-backends", "all-kernels"]

# 所有后端（运行时检测，动态加载）
all-backends = []

# 所有自定义 kernel
all-kernels = []

# Fusion 支持（可选优化）
fusion = ["burn/fusion", "burn-fusion"]

# NCCL 多 GPU（需要 CUDA 环境）
nccl = ["cudarc/nccl"]

# RCCL 多 GPU（需要 ROCm 环境）
rccl = []

# Flash Attention v3 优化（Hopper+）
flash-attention-v3 = [
    "flash-attention-v3-wgmma",
    "flash-attention-v3-async",
    "flash-attention-v3-fp8",
    "flash-attention-v3-block-quant",
]
flash-attention-v3-wgmma = []
flash-attention-v3-async = []
flash-attention-v3-fp8 = []
flash-attention-v3-block-quant = []

# 精简版（仅 CPU，用于测试/CI）
minimal = []

# Backend-specific kernel features (for conditional compilation)
cpu = []
cuda = []
cuda-kernel = []
rocm-kernel = []
metal-kernel = []
wgpu-kernel = []
fused-kernel = []
mamba-kernel = []
paged-kernel = []
softmax-kernel = []

[dependencies]
# Burn tensor library (always included)
burn = { version = "0.20.0-pre.6", default-features = false }
burn-ndarray = { version = "0.20.0-pre.6" }  # CPU 后端，始终编译
burn-fusion = { version = "0.20.0-pre.6", optional = true }

# CUDA 支持（动态加载，运行时检测）
cudarc = { version = "0.18", default-features = false, features = ["std", "driver", "nvrtc", "f16", "cuda-12090", "dynamic-loading"] }

# WGPU 支持（跨平台 GPU）
wgpu = { version = "26.0" }
pollster = { version = "0.4" }

# 通用依赖
log = "0.4"
crossbeam-channel = "0.5"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
half = "2.7"
libloading = "0.8"  # 动态加载 HIP/ROCm 库
memmap2 = "0.9"     # Engram: memory-mapped embedding tables
bytemuck = { version = "1.24.0", features = ["derive"] }

# 平台特定（条件编译，非 feature flag）
[target.'cfg(target_os = "macos")'.dependencies]
metal = { version = "0.29" }
objc = { version = "0.2" }

[dev-dependencies]
criterion = "0.5"

[[bench]]
name = "attention_bench"
harness = false

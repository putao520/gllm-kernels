# gllm-kernels æ¶æ„è®¾è®¡

## æ¦‚è¿°

gllm-kernels é‡‡ç”¨**è¿è¡Œæ—¶åç«¯é€‰æ‹© + é›¶æˆæœ¬æŠ½è±¡ + Fat Binary**æ¶æ„ï¼š
- **Fat Binary**: ç¼–è¯‘æ—¶åµŒå…¥æ‰€æœ‰åç«¯çš„é¢„ç¼–è¯‘ä¸­é—´æ€
- **è¿è¡Œæ—¶æ£€æµ‹**: å¯åŠ¨æ—¶æ£€æµ‹å¯ç”¨åç«¯ï¼Œæ—  feature flag
- **é›¶æˆæœ¬æ´¾å‘**: enum + match å®ç°é›¶ vtable å¼€é”€
- **Driver API Only**: åªä¾èµ– GPU é©±åŠ¨ï¼Œæ— éœ€å¼€å‘å·¥å…·åŒ…

### ğŸš¨ æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼ˆé“å¾‹ï¼‰

| åŸåˆ™ | è¯´æ˜ | ç†ç”± |
|------|------|------|
| **åªåšåº”ç”¨çº§ç®—å­** | åªå®ç° FlashAttentionã€PagedAttentionã€RMSNorm ç­‰ LLM ä¸“ç”¨ç®—å­ | åº•å±‚é€šç”¨ç®—å­ï¼ˆmatmulã€softmaxï¼‰ç”±ç¡¬ä»¶åº“ï¼ˆcuBLASã€MPSï¼‰æ›´é«˜æ•ˆ |
| **ä¸åšåº•å±‚é€šç”¨ç®—å­** | âŒ ç¦æ­¢å®ç°é€šç”¨ matmulã€gemmã€conv2d ç­‰ | è‡ªå·±å®ç°æ€§èƒ½å¿…ç„¶å·®äºç¡¬ä»¶å‚å•†ä¼˜åŒ– |
| **CPU æ˜¯å…œåº•åç«¯** | CPU å®ç°ä»…ä½œ fallbackï¼Œä¸æ˜¯æ€§èƒ½ç›®æ ‡ | GPU æ‰æ˜¯ä¸»æˆ˜åœºï¼ŒCPU åªä¿è¯å¯ç”¨æ€§ |
| **å…¨æµç¨‹å•åç«¯** | GPU ä»»åŠ¡å…¨ç¨‹ GPUï¼ŒCPU ä»»åŠ¡å…¨ç¨‹ CPUï¼Œç¦æ­¢æ··åˆ | GPUâ†”CPU æ•°æ®ä¼ è¾“å¼€é”€æå¤§ï¼Œä¼šæŠµæ¶ˆè®¡ç®—åŠ é€Ÿ |

**å…¨æµç¨‹å•åç«¯åŸåˆ™è¯¦è§£**ï¼š

```
âœ… æ­£ç¡®ï¼šå…¨æµç¨‹ GPU
   upload â†’ flash_attn(GPU) â†’ rms_norm(GPU) â†’ linear(GPU) â†’ readback

âœ… æ­£ç¡®ï¼šå…¨æµç¨‹ CPUï¼ˆæ—  GPU æ—¶ï¼‰
   flash_attn(CPU) â†’ rms_norm(CPU) â†’ linear(CPU)

âŒ é”™è¯¯ï¼šæ··åˆæ‰§è¡Œ
   flash_attn(GPU) â†’ readback â†’ rms_norm(CPU) â†’ upload â†’ linear(GPU)
   â†‘ æ¯æ¬¡ readback/upload éƒ½æ˜¯å·¨å¤§å¼€é”€ï¼
```

---

## æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        gllm-kernels                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Public API Layer                                                    â”‚
â”‚  â”œâ”€â”€ KernelDispatcher         â†’ é›¶æˆæœ¬ç®—å­æ´¾å‘                       â”‚
â”‚  â”œâ”€â”€ detect_backend()         â†’ è¿è¡Œæ—¶æ£€æµ‹ï¼ˆCUDA>ROCm>Metal>WGPU>CPUï¼‰â”‚
â”‚  â””â”€â”€ BackendType enum         â†’ åç«¯ç±»å‹æšä¸¾                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Application-Level Operatorsï¼ˆåªåš LLM ä¸“ç”¨ç®—å­ï¼‰                     â”‚
â”‚  â”œâ”€â”€ flash_attention          â†’ Flash Attention v2/v3               â”‚
â”‚  â”œâ”€â”€ paged_attention          â†’ PagedKV Attention (vLLM)            â”‚
â”‚  â”œâ”€â”€ flash_tree_attention     â†’ Tree-structured Attention           â”‚
â”‚  â”œâ”€â”€ moe_forward              â†’ MoE å‰å‘è®¡ç®—                         â”‚
â”‚  â”œâ”€â”€ rms_norm / layer_norm    â†’ å½’ä¸€åŒ–å±‚                             â”‚
â”‚  â”œâ”€â”€ rope                     â†’ æ—‹è½¬ä½ç½®ç¼–ç                          â”‚
â”‚  â”œâ”€â”€ softmax                  â†’ Log-Space Softmaxï¼ˆæ•°å€¼ç¨³å®šï¼‰        â”‚
â”‚  â””â”€â”€ sampling                 â†’ Top-K/Top-P é‡‡æ ·                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPU Backend Kernelsï¼ˆä¸»æˆ˜åœºï¼‰                                        â”‚
â”‚  â”œâ”€â”€ cuda_kernels/            â†’ PTX + CUDA Driver API                â”‚
â”‚  â”œâ”€â”€ hip_kernels/             â†’ HSACO + HSA Runtime (Driver API)     â”‚
â”‚  â”œâ”€â”€ metal_kernels/           â†’ metallib + Metal Framework           â”‚
â”‚  â””â”€â”€ wgpu_kernels/            â†’ WGSL + wgpu                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CPU Fallbackï¼ˆå…œåº•å®ç°ï¼Œéæ€§èƒ½ç›®æ ‡ï¼‰                                 â”‚
â”‚  â””â”€â”€ ops/                     â†’ çº¯ Rust å‚è€ƒå®ç°ï¼ˆæ—  GPU æ—¶ä½¿ç”¨ï¼‰     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Runtime Detection Layer                                             â”‚
â”‚  â”œâ”€â”€ runtime_detection.rs     â†’ åç«¯å¯ç”¨æ€§æ£€æµ‹                       â”‚
â”‚  â””â”€â”€ kernel_dispatcher.rs     â†’ ç»Ÿä¸€æ´¾å‘å…¥å£                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> âš ï¸ **å·²åºŸå¼ƒ/æœªå®ç°çš„ç®—å­**ï¼šring_attentionã€mlaã€mambaï¼ˆæ€§èƒ½ä¸æ»¡è¶³è¦æ±‚æˆ–æ— éœ€æ±‚ï¼‰

---

## æ ¸å¿ƒæ¶æ„å†³ç­–

### ARCH-BACKEND-001: ç®€åŒ–åç«¯æ¶æ„ï¼ˆğŸš¨ é“å¾‹ - 2026-01ï¼‰

**æ ¸å¿ƒç†å¿µ**ï¼šBackend = ç¡¬ä»¶ç®—æ³•å·¥å…·åº“ï¼Œå°±è¿™ä¹ˆç®€å•ã€‚

**è®¾è®¡åŸåˆ™**ï¼š
- å¯åŠ¨æ—¶é€‰æ‹©ä¸€æ¬¡åç«¯ï¼Œä¹‹åç›´æ¥ä½¿ç”¨
- æ¯ä¸ªåç«¯æ˜¯ç‹¬ç«‹çš„ `Backend` trait å®ç°
- çƒ­è·¯å¾„å†…æ— ä»»ä½•é—´æ¥è°ƒç”¨
- `dyn Trait` åªå…è®¸åœ¨å¯åŠ¨æ—¶ï¼ˆä¸€æ¬¡ vtable æŸ¥æ‰¾å¯å¿½ç•¥ï¼‰

#### æ­£ç¡®æ¶æ„

```rust
// 1. Backend trait - åå‡ ä¸ªæ ¸å¿ƒåº”ç”¨çº§ç®—å­
pub trait Backend: Send + Sync {
    fn flash_attention(&self, q: &GpuTensor, k: &GpuTensor, v: &GpuTensor,
                       output: &mut GpuTensor, config: FlashAttentionConfig) -> Result<(), String>;
    fn paged_attention(&self, ...) -> Result<(), String>;
    fn moe_forward(&self, ...) -> Result<(), String>;
    fn rms_norm(&self, ...) -> Result<(), String>;
    fn linear_forward(&self, ...) -> Result<(), String>;
    fn upload<T>(&self, host: &[T], gpu: &mut GpuTensor) -> Result<(), String>;
    fn readback<T>(&self, gpu: &GpuTensor, host: &mut [T]) -> Result<(), String>;
    // ~15 ä¸ªæ ¸å¿ƒç®—å­ï¼Œä¸éœ€è¦æ›´å¤š
}

// 2. æ¯ä¸ªåç«¯å„è‡ªå®ç° - ç‹¬ç«‹æ–‡ä»¶
// wgpu_backend.rs
impl Backend for WgpuBackend { ... }

// cuda_backend.rs
impl Backend for CudaBackend { ... }

// cpu_backend.rs
impl Backend for CpuBackend { ... }

// 3. å¯åŠ¨æ—¶é€‰ä¸€æ¬¡ï¼Œç›´æ¥ç”¨
pub fn auto_select_backend() -> Arc<dyn Backend> {
    if cuda_available() { return Arc::new(CudaBackend::new()); }
    if rocm_available() { return Arc::new(RocmBackend::new()); }
    if metal_available() { return Arc::new(MetalBackend::new()); }
    if wgpu_available() { return Arc::new(WgpuBackend::new()); }
    Arc::new(CpuBackend::new())
}

// 4. ä½¿ç”¨ - ä¸€æ¬¡åŠ¨æ€åˆ†å‘ï¼Œå®Œäº‹
let backend = auto_select_backend();
backend.flash_attention(...);  // ç›´æ¥è°ƒç”¨ï¼Œæ²¡æœ‰ä¸­é—´å±‚
```

#### å®ç°çŠ¶æ€ï¼ˆ2026-01-26 æ›´æ–°ï¼‰

**é˜¶æ®µ1 å·²å®Œæˆ** âœ…ï¼šå¤–å±‚æ¶æ„é‡ç»„

| ç›®æ ‡ | é‡æ„å‰ | é‡æ„å | çŠ¶æ€ |
|------|--------|--------|------|
| æ´¾å‘æœºåˆ¶ | `DispatchedBackend` enum | `Arc<dyn Backend>` | âœ… |
| backend.rs è¡Œæ•° | 6,270 è¡Œ | 20 è¡Œ | âœ… |
| åç«¯æ–‡ä»¶ | å…¨åœ¨ backend.rs | ç‹¬ç«‹æ–‡ä»¶ | âœ… |
| `auto_select_backend()` | æ—  | è¿”å› `Arc<dyn Backend>` | âœ… |
| Backend trait æ–¹æ³•æ•° | 52 ä¸ª | 15 ä¸ª | âœ… |

**é˜¶æ®µ2 å¾…å®Œæˆ** âš ï¸ï¼šæ¶ˆé™¤ KernelDispatcher ä¸­é—´å±‚

**å½“å‰é—®é¢˜**ï¼š
```
Backend trait
    â†“ å§”æ‰˜
BackendCore
    â†“ å§”æ‰˜
KernelDispatcherï¼ˆ7,646 è¡Œï¼‰
    â†“ match self.backend åˆ†å‘ï¼ˆæ¯ä¸ªç®—å­å†…éƒ¨ï¼‰
cuda_kernels / wgpu_kernels / metal_kernels / ops
```

KernelDispatcher å†…éƒ¨ä»æœ‰å¤§é‡ `match self.backend` åˆ†å‘ï¼Œè¿™è¿åäº†"å¯åŠ¨æ—¶é€‰ä¸€æ¬¡"åŸåˆ™ã€‚

**æ­£ç¡®æ¶æ„**ï¼ˆé˜¶æ®µ2ç›®æ ‡ï¼‰ï¼š
```
Backend trait
    â†“ ç›´æ¥å®ç°
CpuBackend   â†’ ç›´æ¥è°ƒç”¨ ops/
CudaBackend  â†’ ç›´æ¥è°ƒç”¨ cuda_kernels/
WgpuBackend  â†’ ç›´æ¥è°ƒç”¨ wgpu_kernels/
MetalBackend â†’ ç›´æ¥è°ƒç”¨ metal_kernels/
RocmBackend  â†’ ç›´æ¥è°ƒç”¨ hip_kernels/
```

**é˜¶æ®µ2 ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æå– KernelDispatcher ä¸­çš„ CPU å®ç°åˆ° `CpuBackend`
- [ ] æå– CUDA åˆ†å‘é€»è¾‘åˆ° `CudaBackend`
- [ ] æå– WGPU åˆ†å‘é€»è¾‘åˆ° `WgpuBackend`
- [ ] æå– Metal åˆ†å‘é€»è¾‘åˆ° `MetalBackend`
- [ ] æå– ROCm åˆ†å‘é€»è¾‘åˆ° `RocmBackend`
- [ ] åˆ é™¤ KernelDispatcher æˆ–å°†å…¶é™çº§ä¸ºçº¯æµ‹è¯•å·¥å…·
- [ ] åˆ é™¤ BackendCore ä¸­é—´å±‚

#### ç®—å­å®ç°çŠ¶æ€çŸ©é˜µï¼ˆKernelDispatcher å†…éƒ¨ï¼‰

**è¯´æ˜**ï¼šä»¥ä¸‹çŸ©é˜µè®°å½• KernelDispatcher å†…éƒ¨å„ç®—å­åœ¨ä¸åŒåç«¯çš„å®ç°çŠ¶æ€ã€‚é˜¶æ®µ2å®Œæˆåï¼Œè¿™äº›å®ç°å°†è¿ç§»åˆ°å„ç‹¬ç«‹ Backend æ–‡ä»¶ä¸­ã€‚

##### Backend trait æ ¸å¿ƒç®—å­ï¼ˆ15ä¸ªï¼‰

| ç®—å­ | CPU | CUDA | Metal | WGPU | ROCm | è¯´æ˜ |
|------|-----|------|-------|------|------|------|
| flash_attention | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | FlashAttention-2 |
| paged_attention | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | PagedAttention |
| softmax | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | ç‹¬ç«‹ softmax |
| matmul | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | é€šç”¨çŸ©é˜µä¹˜ |
| rope_precompute | âœ… | âœ… | âœ… | âœ… | âœ… | RoPE é¢„è®¡ç®— |
| rope_apply | âœ… | âœ… | âœ… | âœ… | âœ… | RoPE åº”ç”¨ |
| rope_apply_inplace | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | RoPE åŸåœ°åº”ç”¨ |
| topk | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | Top-K é‡‡æ · |
| apply_temperature | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | æ¸©åº¦ç¼©æ”¾ |
| sample_tokens | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | Token é‡‡æ · |
| argmax | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | è´ªå©ªè§£ç  |
| moe_route | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | MoE è·¯ç”± |
| compute_routing_logits | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | è·¯ç”± logits |
| add_bias | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | åç½®åŠ æ³• |
| backend_type | âœ… | âœ… | âœ… | âœ… | âœ… | ç±»å‹æŸ¥è¯¢ |

##### KernelDispatcher å…¶ä»–ç®—å­

| ç®—å­ | CPU | CUDA | Metal | WGPU | ROCm | è¯´æ˜ |
|------|-----|------|-------|------|------|------|
| rms_norm | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | RMS å½’ä¸€åŒ– |
| layer_norm | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | Layer å½’ä¸€åŒ– |
| silu | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | SiLU æ¿€æ´» |
| gelu | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | GELU æ¿€æ´» |
| embedding | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | è¯åµŒå…¥æŸ¥è¡¨ |
| causal_mask | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | å› æœæ©ç  |
| quantize | âœ… | âœ… | âš ï¸ partial | âš ï¸ partial | âŒ | é‡åŒ–æ“ä½œ |
| dequantize | âœ… | âœ… | âš ï¸ partial | âš ï¸ partial | âŒ | åé‡åŒ–æ“ä½œ |
| fused_attention | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | èåˆæ³¨æ„åŠ› |
| moe_forward | âœ… | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | âš ï¸ CPU | MoE å‰å‘ |
| flash_tree_attention | âœ… | âš ï¸ partial | âŒ | âŒ | âŒ | æ ‘ç»“æ„æ³¨æ„åŠ› |
| concat | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | å¼ é‡æ‹¼æ¥ |
| split | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | å¼ é‡åˆ†å‰² |
| transpose | âœ… | âœ… | âœ… | âœ… | âš ï¸ fallback | è½¬ç½® |
| copy | âœ… | âœ… | âœ… | âœ… | âœ… | å†…å­˜æ‹·è´ |

##### å®ç°è¦†ç›–ç‡ç»Ÿè®¡

| åç«¯ | å®Œæ•´å®ç° | CPU Fallback | éƒ¨åˆ†å®ç° | æœªå®ç° | è¦†ç›–ç‡ |
|------|---------|--------------|----------|--------|--------|
| CPU | 30/30 | - | - | 0 | **100%** |
| CUDA | 18/30 | 10 | 2 | 0 | **60%** (å«fallback: 100%) |
| Metal | 16/30 | 10 | 2 | 2 | **53%** (å«fallback: 93%) |
| WGPU | 16/30 | 10 | 2 | 2 | **53%** (å«fallback: 93%) |
| ROCm | 2/30 | 26 | 0 | 2 | **7%** (å«fallback: 93%) |

**å›¾ä¾‹**ï¼š
- âœ… = åŸç”Ÿ GPU/CPU å®ç°
- âš ï¸ CPU = å›é€€åˆ° CPU å®ç°
- âš ï¸ fallback = å›é€€åˆ°å…¶ä»–åç«¯
- âš ï¸ partial = éƒ¨åˆ†å®ç°
- âŒ = æœªå®ç°

**é˜¶æ®µ2ä¼˜å…ˆçº§**ï¼ˆåŸºäºè¦†ç›–ç‡ï¼‰ï¼š
1. CPUï¼ˆ100%ï¼‰â†’ ç›´æ¥è¿ç§»åˆ° CpuBackend
2. CUDAï¼ˆ60%ï¼‰â†’ è¿ç§»å·²æœ‰å®ç°ï¼Œä¿æŒ CPU fallback
3. Metal/WGPUï¼ˆ53%ï¼‰â†’ è¿ç§»å·²æœ‰å®ç°
4. ROCmï¼ˆ7%ï¼‰â†’ è¿ç§» RoPEï¼Œå…¶ä»–ä¿æŒ fallback

#### å…³äº `dyn Trait`

| åœºæ™¯ | æ˜¯å¦å…è®¸ | åŸå›  |
|------|----------|------|
| å¯åŠ¨æ—¶é€‰ä¸€æ¬¡åç«¯ | âœ… å…è®¸ | ä¸€æ¬¡ vtable æŸ¥æ‰¾ï¼Œå¿½ç•¥ä¸è®¡ |
| çƒ­è·¯å¾„å†…éƒ¨ï¼ˆæ¯æ¬¡ matmulï¼‰ | âŒ ç¦æ­¢ | vtable = è¿è¡Œæ—¶å¼€é”€ |

#### å½“å‰æ–‡ä»¶ç»“æ„ï¼ˆé˜¶æ®µ1å®Œæˆåï¼‰

```
src/
â”œâ”€â”€ backend.rs              # å…¥å£æ¨¡å— + auto_select_backend()ï¼ˆ20 è¡Œï¼‰
â”œâ”€â”€ backend_trait.rs        # Backend trait å®šä¹‰ï¼ˆ15 ä¸ªæ–¹æ³•ï¼‰
â”œâ”€â”€ backend_core.rs         # âš ï¸ ä¸­é—´å±‚ï¼Œå¾…åˆ é™¤
â”œâ”€â”€ backend_core_*.rs       # âš ï¸ ä¸­é—´å±‚ï¼Œå¾…åˆ é™¤
â”œâ”€â”€ cpu_backend.rs          # CpuBackendï¼ˆå§”æ‰˜åˆ° BackendCoreï¼‰
â”œâ”€â”€ cuda_backend.rs         # CudaBackendï¼ˆå§”æ‰˜åˆ° BackendCoreï¼‰
â”œâ”€â”€ wgpu_backend.rs         # WgpuBackendï¼ˆå§”æ‰˜åˆ° BackendCoreï¼‰
â”œâ”€â”€ metal_backend.rs        # MetalBackendï¼ˆå§”æ‰˜åˆ° BackendCoreï¼‰
â”œâ”€â”€ rocm_backend.rs         # RocmBackendï¼ˆå§”æ‰˜åˆ° BackendCoreï¼‰
â”‚
â”œâ”€â”€ kernel_dispatcher.rs    # âš ï¸ 7,646 è¡Œï¼ŒåŒ…å«å®é™…åˆ†å‘é€»è¾‘ï¼Œå¾…æ‹†è§£
â”‚
â”œâ”€â”€ cuda_kernels/           # CUDA GPU kernel å®ç°
â”œâ”€â”€ hip_kernels/            # ROCm GPU kernel å®ç°
â”œâ”€â”€ metal_kernels/          # Metal GPU kernel å®ç°
â”œâ”€â”€ wgpu_kernels/           # WGPU GPU kernel å®ç°
â”‚
â””â”€â”€ ops/                    # CPU å®ç°ï¼ˆçº¯ Rust + SIMDï¼‰
```

---

### ARCH-SCOPE-001: ç®—å­å®ç°èŒƒå›´ï¼ˆğŸš¨ é“å¾‹ï¼‰

**æˆ‘ä»¬åªå®ç° LLM æ¨ç†ä¸“ç”¨çš„åº”ç”¨çº§ç®—å­ï¼Œä¸å®ç°åº•å±‚é€šç”¨ç®—å­ã€‚**

#### âœ… å®ç°çš„ç®—å­ï¼ˆåº”ç”¨çº§ï¼ŒLLM ä¸“ç”¨ï¼‰

| ç®—å­ | ç”¨é€” | è¯´æ˜ |
|------|------|------|
| flash_attention | Transformer æ³¨æ„åŠ› | FlashAttention-2/3 ç®—æ³• |
| paged_attention | KV Cache åˆ†é¡µ | vLLM PagedAttention |
| flash_tree_attention | æ ‘ç»“æ„æ³¨æ„åŠ› | æ¨æµ‹è§£ç ç”¨ |
| moe_forward | MoE ä¸“å®¶å‰å‘ | Mixtral/DeepSeek ç­‰ |
| rms_norm | RMS å½’ä¸€åŒ– | LLaMA ç­‰æ¨¡å‹ |
| layer_norm | Layer å½’ä¸€åŒ– | GPT/BERT ç­‰æ¨¡å‹ |
| rope | æ—‹è½¬ä½ç½®ç¼–ç  | å¤§å¤šæ•°ç°ä»£ LLM |
| silu/gelu | æ¿€æ´»å‡½æ•° | LLM å¸¸ç”¨æ¿€æ´» |
| sampling | Top-K/Top-P é‡‡æ · | æ–‡æœ¬ç”Ÿæˆ |

#### âŒ ä¸å®ç°çš„ç®—å­ï¼ˆåº•å±‚é€šç”¨ï¼‰

| ç®—å­ | åŸå›  |
|------|------|
| matmul / gemm | cuBLAS / MPS / BLAS å·²æè‡´ä¼˜åŒ–ï¼Œè‡ªå·±å†™å¿…ç„¶æ›´æ…¢ |
| conv2d / pooling | å›¾åƒç®—å­ï¼Œä¸ LLM æ— å…³ |
| batch_norm | CV ç”¨ï¼ŒLLM ç”¨ RMSNorm/LayerNorm |
| é€šç”¨ softmax | å·²èåˆåˆ° FlashAttention å†…éƒ¨ |
| transpose / reshape | é›¶æˆæœ¬è§†å›¾æ“ä½œï¼Œä¸éœ€è¦ kernel |

#### è®¾è®¡ç†ç”±

```
âŒ é”™è¯¯æ€è·¯ï¼š
"æˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªé«˜æ€§èƒ½ matmul kernel"
â†’ NVIDIA èŠ±äº† 20 å¹´ä¼˜åŒ– cuBLASï¼Œä½ ä¸å¯èƒ½è¶…è¶Š

âœ… æ­£ç¡®æ€è·¯ï¼š
"æˆ‘ä»¬å®ç° FlashAttentionï¼Œå†…éƒ¨è°ƒç”¨ cuBLAS matmul"
â†’ ç®—æ³•åˆ›æ–° + å¤ç”¨ç¡¬ä»¶åº“ = æœ€ä½³æ€§èƒ½
```

#### åºŸå¼ƒçš„ç®—å­

| ç®—å­ | çŠ¶æ€ | åŸå›  |
|------|------|------|
| ring_attention | âŒ åºŸå¼ƒ | åˆ†å¸ƒå¼åœºæ™¯æœªéªŒè¯ï¼Œæš‚ä¸éœ€è¦ |
| mla | âŒ åºŸå¼ƒ | ä¸ flash_attention åˆå¹¶å¤„ç† |
| mamba | âŒ åºŸå¼ƒ | SSM æ¨¡å‹æ”¯æŒä¼˜å…ˆçº§ä½ |

---

### ARCH-ROUTE-001: ä¼˜åŒ–è·¯ç”±å†³ç­–ï¼ˆå°æ¨¡å‹/å°æ‰¹é‡ï¼‰

**æ ¸å¿ƒé—®é¢˜**ï¼šGPU ä¸æ˜¯ä¸‡èƒ½çš„ï¼ŒæŸäº›åœºæ™¯ä¸‹ CPU åè€Œæ›´å¿«ã€‚

#### 1. æ¨¡å‹çº§è·¯ç”±ï¼š800M å‚æ•°é˜ˆå€¼

**å®ç°ä½ç½®**ï¼š`gllm/src/engine.rs:588-614`

```rust
// è‡ªåŠ¨è·¯ç”±é€»è¾‘ï¼ˆDevice::Auto æ—¶ç”Ÿæ•ˆï¼‰
if let Ok((config, _)) = ModelConfig::load(repo_name, Some(&config_path)) {
    let params = estimate_model_params(&config);
    // é˜ˆå€¼: 800M å‚æ•°ã€‚ä½äºæ­¤å€¼æ—¶ï¼ŒCPU å¯¹å°æ‰¹é‡æ›´å¿«ï¼Œä¸”èŠ‚çœ GPU æ˜¾å­˜
    if params < 800_000_000 {
        log::debug!("Auto-routing small model ({} params) to CPU", params);
        return false;  // ä½¿ç”¨ CPU
    }
}
return true;  // ä½¿ç”¨ GPU
```

**å‚æ•°ä¼°ç®—å…¬å¼**ï¼š

```rust
fn estimate_model_params(config: &ModelConfig) -> usize {
    let hidden = config.hidden_size;
    let vocab = config.vocab_size;
    let layers = config.num_hidden_layers;

    // Embedding å‚æ•°: vocab Ã— hidden
    let embedding_params = vocab * hidden;
    // Transformer å±‚å‚æ•°: layers Ã— 12 Ã— hiddenÂ²
    // (4Ã—hÂ² attention + 8Ã—hÂ² MLP çš„è¿‘ä¼¼)
    let layer_params = layers * 12 * hidden * hidden;

    embedding_params + layer_params
}
```

**è·¯ç”±å†³ç­–è¡¨**ï¼š

| æ¨¡å‹è§„æ¨¡ | å‚æ•°é‡ | è·¯ç”±ç›®æ ‡ | åŸå›  |
|----------|--------|----------|------|
| å°æ¨¡å‹ | < 800M | CPU | è®¡ç®—å¯†åº¦ä½ï¼ŒPCIe å¼€é”€å ä¸»å¯¼ |
| ä¸­å¤§æ¨¡å‹ | â‰¥ 800M | GPU | è®¡ç®—é‡å¤§ï¼ŒGPU å¹¶è¡Œä¼˜åŠ¿æ˜æ˜¾ |

#### 2. Decode Mode ä¼˜åŒ–ï¼šå• Token ç”Ÿæˆ

**å®ç°ä½ç½®**ï¼š`gllm/src/causal_attention.rs:428-437`

```rust
// Decode é˜¶æ®µï¼šæ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª token
self.dispatcher.flash_attention(
    q_buf, cached_k, cached_v, attn_out_buf,
    FlashAttentionConfig {
        causal: true,
        num_heads: self.num_attention_heads,
        head_dim: self.head_dim,
        seq_len_q: 1,        // åªæœ‰ 1 ä¸ªæŸ¥è¯¢ token
        seq_len_kv: key_len, // KV cache é•¿åº¦
        batch_size: 1,       // å•æ‰¹æ¬¡
        ..Default::default()
    },
);
```

**åœºæ™¯è¯´æ˜**ï¼š
- `seq_len_q: 1` = å• token æŸ¥è¯¢ï¼ˆè‡ªå›å½’ç”Ÿæˆçš„æ¯ä¸€æ­¥ï¼‰
- `batch_size: 1` = å•è¯·æ±‚æ¨ç†
- æ­¤é…ç½®ä¸‹ GPU kernel launch å¼€é”€å¯èƒ½æ¥è¿‘å®é™…è®¡ç®—æ—¶é—´

#### 3. ç®—å­çº§è·¯ç”±ï¼šå§‹ç»ˆ CPU çš„æ“ä½œ

**å®ç°ä½ç½®**ï¼š`gllm-kernels/src/kernel_dispatcher.rs:2995-3009`

```rust
pub fn matryoshka_truncate(&self, embeddings: &[f32], output: &mut [f32], config: MatryoshkaConfig) {
    // ALWAYS use CPU - GPU overhead exceeds computation time
    // for this simple memory-bound operation (dimension truncation + optional normalize)
    crate::ops::embedding::matryoshka_truncate(embeddings, output, &config);
}
```

**å§‹ç»ˆ CPU çš„ç®—å­**ï¼š

| ç®—å­ | åŸå›  | GPU å¼€é”€ |
|------|------|----------|
| `matryoshka_truncate` | çº¯å†…å­˜æ‹·è´ + å¯é€‰ L2 å½’ä¸€åŒ– | kernel launch (~5-50Î¼s) + PCIe (~100-400Î¼s) > è®¡ç®—æ—¶é—´ |

#### 4. CPU SIMD åŠ é€Ÿè·¯å¾„

**æ£€æµ‹ä½ç½®**ï¼š`ops/simd_asm.rs::current_simd_path()`

| æ¶æ„ | SIMD æŒ‡ä»¤é›† | ååé‡ |
|------|------------|--------|
| x86_64 | AVX-512 | 64 floats/iter |
| x86_64 | AVX2+FMA | 32 floats/iter |
| ARM64 | NEON | 16 floats/iter |
| é€šç”¨ | wide crate | 8 floats/iter |

---

### ARCH-KERNELS-001: Kernel ç›®å½•ç»“æ„

> **âš ï¸ è¯´æ˜**ï¼šä»¥ä¸‹æ˜¯åŸºç¡€ç»“æ„è§„èŒƒï¼Œå®é™…å®ç°åŒ…å«æ›´å¤š kernel æ–‡ä»¶ï¼ˆeagle3ã€medusaã€spec_eeã€chunked_prefillã€evic_pressã€int2_quantizerã€prompt_cacheã€flash_tree_attnã€moe_ffnã€linearã€rms_norm ç­‰ï¼‰

**æ¯ä¸ªåç«¯çš„ kernel æ¨¡å—åŸºæœ¬ç›®å½•ç»“æ„**ï¼š

```
src/{backend}_kernels/
â”œâ”€â”€ mod.rs                      # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ {runtime}.rs                # Runtime æŠ½è±¡ï¼ˆå‘½åå› åç«¯è€Œå¼‚ï¼‰
â”œâ”€â”€ flash_attn.rs               # Flash Attention Kernel
â”œâ”€â”€ paged_attn.rs               # Paged Attention Kernel
â”œâ”€â”€ [å…¶ä»– kernel].rs            # æ›´å¤š kernel å®ç°...
â””â”€â”€ kernels/                    # é¢„ç¼–è¯‘ä¸­é—´æ€
    â”œâ”€â”€ flash_attention.{ext}   # åµŒå…¥çš„ä¸­é—´æ€
    â””â”€â”€ [å…¶ä»– kernel].{ext}     # æ›´å¤šé¢„ç¼–è¯‘ä¸­é—´æ€
```

**ä¸­é—´æ€æ ¼å¼å¯¹ç…§è¡¨**ï¼š

| åç«¯ | ä¸­é—´æ€æ‰©å±•å | æºç æ‰©å±•å | ç¼–è¯‘å·¥å…· | è¿è¡Œæ—¶ä¾èµ– |
|------|-------------|-----------|----------|-----------|
| CUDA | `.ptx` | `.cu` | `nvcc -ptx` | `libcuda.so` |
| ROCm | `.hsaco` | `.hip` | `hipcc --genco` | `libhsa-runtime64.so` |
| Metal | `.metallib` | `.metal` | `xcrun metallib` | Metal.framework |
| WGPU | `.wgsl` | N/A | N/Aï¼ˆWGSL å³ä¸­é—´æ€ï¼‰ | wgpu |
| CPU | N/A | N/A | N/A | æ—  |

**è¯´æ˜**ï¼šWGSL æ˜¯ WebGPU çš„æ ‡å‡†ç€è‰²å™¨è¯­è¨€ï¼Œæœ¬èº«å°±æ˜¯ä¸­é—´æ€æ ¼å¼ï¼Œæ— éœ€é¢„ç¼–è¯‘ã€‚é€šè¿‡ `include_str!` åµŒå…¥åç”± wgpu è¿è¡Œæ—¶ç¼–è¯‘ä¸ºå„å¹³å°åŸç”Ÿæ ¼å¼ã€‚

### ARCH-KERNELS-002: Runtime æ¥å£å¥‘çº¦

> **âš ï¸ è¯´æ˜**ï¼šä»¥ä¸‹æ˜¯æ ¸å¿ƒæ¥å£è§„èŒƒï¼Œå„åç«¯å®é™…å®ç°å¯èƒ½æœ‰å·®å¼‚ï¼ˆå¦‚ CUDA ä½¿ç”¨ cudarcã€ROCm ä½¿ç”¨ HSA Runtimeï¼‰

**æ¯ä¸ªåç«¯çš„ `runtime` æ¨¡å—åº”æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½**ï¼š

#### æ£€æµ‹æ¥å£

| æ–¹æ³• | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|----------|------|
| `is_available()` | bool | æ£€æµ‹åç«¯æ˜¯å¦å¯ç”¨ï¼ˆé©±åŠ¨å·²å®‰è£…ä¸”èƒ½æ­£å¸¸åˆå§‹åŒ–ï¼‰ |
| `device_count()` | usize | è·å–å¯ç”¨è®¾å¤‡æ•°é‡ |

#### Buffer ç®¡ç†æ¥å£

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `Buffer.new` | device: usize, size: usize | Result\<Buffer, Error\> | åœ¨æŒ‡å®šè®¾å¤‡ä¸Šåˆ†é…å†…å­˜ |
| `Buffer.from_slice` | device: usize, data: slice | Result\<Buffer, Error\> | ä» host æ•°æ®åˆ›å»ºå¹¶å¤åˆ¶åˆ° GPU |
| `Buffer.to_vec` | - | Result\<Vec, Error\> | ä» GPU å¤åˆ¶æ•°æ®å› host |
| `Buffer.len` | - | usize | è·å– buffer é•¿åº¦ï¼ˆå…ƒç´ æ•°é‡ï¼‰ |

#### Kernel åŠ è½½æ¥å£ï¼ˆé›¶é…ç½®ï¼‰

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `KernelLibrary.from_bytes` | data: bytes | Result\<KernelLibrary, Error\> | ä»åµŒå…¥çš„å­—èŠ‚åŠ è½½ï¼ˆä¸»è¦è·¯å¾„ï¼‰ |
| `KernelLibrary.get_function` | name: string | Result\<Function, Error\> | è·å– kernel å‡½æ•° |

**ğŸš¨ ç¦æ­¢çš„æ¥å£**ï¼š
- âŒ `compile_source(source)` - ç¦æ­¢ä»æºç ç¼–è¯‘

### ARCH-KERNELS-003: Kernel æ¥å£å¥‘çº¦

> **âš ï¸ è¯´æ˜**ï¼šä»¥ä¸‹æ˜¯æ ¸å¿ƒ Attention Kernel çš„æ¥å£è§„èŒƒç¤ºä¾‹ï¼Œå®é™…å®ç°é€šè¿‡ `Backend` trait ç»Ÿä¸€è°ƒåº¦

#### FlashAttentionKernel æ¥å£

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `new(device)` | åˆ›å»º Kernel å®ä¾‹ï¼ˆåŠ è½½é¢„ç¼–è¯‘ä¸­é—´æ€ï¼‰ |
| `forward_f32(...)` | f32 å‰å‘è®¡ç®— |
| `forward_f16(...)` | f16 å‰å‘è®¡ç®— |

**forward å‚æ•°è¡¨**ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| q | Buffer | Query: [batch, heads, seq_len, head_dim] |
| k | Buffer | Key: [batch, heads, seq_len, head_dim] |
| v | Buffer | Value: [batch, heads, seq_len, head_dim] |
| batch_size | usize | æ‰¹å¤§å° |
| num_heads | usize | æ³¨æ„åŠ›å¤´æ•° |
| seq_len | usize | åºåˆ—é•¿åº¦ |
| head_dim | usize | å¤´ç»´åº¦ |
| is_causal | bool | æ˜¯å¦å› æœæ©ç  |
| scale | f32 | ç¼©æ”¾å› å­ |

#### PagedAttentionKernel æ¥å£

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `new(device)` | åˆ›å»º Kernel å®ä¾‹ |
| `forward_f32(...)` | f32 å‰å‘è®¡ç®— |
| `forward_f16(...)` | f16 å‰å‘è®¡ç®— |

**forward å‚æ•°è¡¨**ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| q | Buffer | Query |
| k_cache | Buffer | Key ç¼“å­˜ |
| v_cache | Buffer | Value ç¼“å­˜ |
| block_tables | Buffer\<i32\> | å—ç´¢å¼•è¡¨ |
| block_offsets | Buffer\<i32\> | å—å†…åç§» |
| batch_size | usize | æ‰¹å¤§å° |
| num_heads | usize | æ³¨æ„åŠ›å¤´æ•° |
| head_dim | usize | å¤´ç»´åº¦ |
| block_size | usize | å—å¤§å° |
| seq_len | usize | åºåˆ—é•¿åº¦ |

---

## ARCH-LOAD-001: ç»Ÿä¸€ Kernel åŠ è½½ç­–ç•¥ï¼ˆğŸš¨ é›¶é…ç½® + Fat Binary + ä¸­é—´æ€è¿è¡Œæ—¶åŠ è½½ï¼‰

**æ ¸å¿ƒè®¾è®¡ç†å¿µ**ï¼š
- **å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œé›¶é…ç½®**ï¼šç”¨æˆ·ä¸éœ€è¦é…ç½®ä»»ä½•ä¸œè¥¿ï¼Œå³æ’å³ç”¨ï¼Œæ— æ„ŸçŸ¥ä½“éªŒ
- **Fat Binary**ï¼šç¼–è¯‘æ—¶åµŒå…¥æ‰€æœ‰å¹³å°çš„é¢„ç¼–è¯‘ä¸­é—´æ€ï¼ˆPTX/HSACO/metallib/WGSLï¼‰
- **ä¸­é—´æ€è¿è¡Œæ—¶åŠ è½½**ï¼šè¿è¡Œæ—¶åŠ è½½é¢„ç¼–è¯‘çš„ä¸­é—´æ€ï¼Œç”± Driver/Runtime è½¬æ¢ä¸ºæœ¬åœ°ä»£ç 
- **ğŸš¨ ç¦æ­¢æºç è¿è¡Œæ—¶ç¼–è¯‘**ï¼šä¸å…è®¸ä» .cu/.hip/.metal æºç è¿è¡Œæ—¶ç¼–è¯‘
- **âŒ ç¦æ­¢ç¯å¢ƒå˜é‡**ï¼šåˆ é™¤æ‰€æœ‰ `GLLM_*` ç¯å¢ƒå˜é‡æ”¯æŒ

**ä¸­é—´æ€ç¼–è¯‘æ¶æ„**ï¼š

```
ç¼–è¯‘æ—¶ï¼ˆCI/å¼€å‘è€…æœºå™¨ï¼‰                     è¿è¡Œæ—¶ï¼ˆç”¨æˆ·æœºå™¨ï¼‰
========================                   ========================
.cu  â”€â”€nvcc -ptxâ”€â”€â†’ PTX      â”€â”
.hip â”€â”€hipcc --gencoâ”€â”€â†’ HSACO  â”œâ”€ include_bytes! â”€â†’ Fat Binary
.metal â”€â”€xcrun metallibâ”€â”€â†’ metallib            â”‚
.wgsl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ WGSL â”€â”˜                â†“
                                           åŠ è½½ä¸­é—´æ€
                                               â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â†“                  â†“                  â†“
                      CUDA Driver         HSA Runtime       Metal/wgpu
                      (PTXâ†’SASS)         (HSACOâ†’æ‰§è¡Œ)    (metallib/WGSLâ†’æ‰§è¡Œ)
```

**ç»Ÿä¸€åŠ è½½æµç¨‹**ï¼š

| æ­¥éª¤ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| 1 | è¯»å–åµŒå…¥çš„ä¸­é—´æ€ | ç¼–è¯‘æ—¶é€šè¿‡ `include_bytes!` åµŒå…¥ |
| 2 | æ£€æŸ¥ä¸­é—´æ€æœ‰æ•ˆæ€§ | å¦‚æœä¸ºç©ºï¼Œè¿”å›é”™è¯¯ |
| 3 | åŠ è½½åˆ° Driver/Runtime | Driver è´Ÿè´£ JIT è½¬æ¢ä¸ºæœ¬åœ°ä»£ç  |

**é”™è¯¯å¤„ç†**ï¼š
- ä¸­é—´æ€ä¸ºç©º â†’ `KernelNotFound` é”™è¯¯
- åŠ è½½å¤±è´¥ â†’ `KernelLoadFailed` é”™è¯¯
- ğŸš¨ æ— å›é€€è·¯å¾„ï¼Œç¦æ­¢æºç ç¼–è¯‘

**å„åç«¯ä¸­é—´æ€åŠ è½½**ï¼š

| åç«¯ | ä¸­é—´æ€ | åµŒå…¥æ–¹å¼ | è¿è¡Œæ—¶åŠ è½½ | æºç ç¼–è¯‘ |
|------|--------|----------|-----------|----------|
| CUDA | PTX | `include_bytes!` | âœ… CUDA Driver JIT | âŒ ç¦æ­¢ |
| ROCm | HSACO | `include_bytes!` | âœ… HSA Runtime | âŒ ç¦æ­¢ |
| Metal | metallib | `include_bytes!` | âœ… Metal Framework | âŒ ç¦æ­¢ |
| WGPU | WGSL | `include_str!` | âœ… wgpu è½¬æ¢ | âŒ ç¦æ­¢ |
| CPU | N/A | N/A | N/A | N/A |

**ğŸš¨ é“å¾‹çº¦æŸ**ï¼š
- âœ… **ä¸­é—´æ€è¿è¡Œæ—¶åŠ è½½**ï¼šPTX/HSACO/metallib/WGSL åœ¨è¿è¡Œæ—¶ç”± Driver/Runtime åŠ è½½æ‰§è¡Œ
- âŒ **ç¦æ­¢æºç è¿è¡Œæ—¶ç¼–è¯‘**ï¼šä¸å…è®¸ä» .cu/.hip/.metal æºç è¿è¡Œæ—¶ç¼–è¯‘
- âŒ **ç¦æ­¢ç¯å¢ƒå˜é‡é…ç½®**ï¼šåˆ é™¤æ‰€æœ‰ `GLLM_*` ç¯å¢ƒå˜é‡
- âŒ **ç¦æ­¢é…ç½®æ–‡ä»¶**ï¼šæ— éœ€ä»»ä½•å¤–éƒ¨é…ç½®æ–‡ä»¶
- âœ… **è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶**ï¼šå¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹ GPU ç±»å‹
- âœ… **è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜**ï¼šæŒ‰æ£€æµ‹é¡ºåºï¼ˆCUDA > ROCm > Metal > WGPU > CPUï¼‰é€‰æ‹©

**ä¸­é—´æ€ vs æºç ç¼–è¯‘çš„åŒºåˆ«**ï¼š

| ç±»å‹ | å®šä¹‰ | ç¤ºä¾‹ | å…è®¸ |
|------|------|------|------|
| ä¸­é—´æ€åŠ è½½ | åŠ è½½é¢„ç¼–è¯‘çš„ä¸­é—´è¡¨ç¤º | PTXâ†’SASS, HSACOâ†’æ‰§è¡Œ, WGSLâ†’SPIR-V | âœ… |
| æºç ç¼–è¯‘ | ä»é«˜çº§è¯­è¨€æºç ç¼–è¯‘ | .cuâ†’PTX, .metalâ†’metallib | âŒ |

**WGSL è¯´æ˜**ï¼šWGSL æ˜¯ WebGPU çš„æ ‡å‡†ä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰ï¼Œwgpu åœ¨è¿è¡Œæ—¶å°†å…¶è½¬æ¢ä¸ºå„å¹³å°åŸç”Ÿæ ¼å¼ï¼ˆVulkan SPIR-Vã€Metal MSLã€DirectX DXILï¼‰ã€‚è¿™æ˜¯"ä¸­é—´æ€åˆ°åŸç”Ÿç çš„è½¬æ¢"ï¼Œç±»ä¼¼ PTX è¢« CUDA Driver JIT ä¸º SASSï¼Œä¸æ˜¯"æºç ç¼–è¯‘"ã€‚

---

## ARCH-ROCM-001: ROCm åç«¯ï¼ˆHSA Runtime Onlyï¼‰

**å†³ç­–**ï¼šåªä½¿ç”¨ HSA Runtime (Driver API)ï¼Œæ—  HIP Runtime

**ç†ç”±**ï¼š
1. HSA Runtime = Driver APIï¼Œåªéœ€ AMD GPU é©±åŠ¨
2. HIP Runtime = Runtime APIï¼Œéœ€è¦å®Œæ•´ ROCm å¼€å‘å·¥å…·åŒ…
3. ä¸ CUDA (cudarc Driver API) å’Œ Metal (Metal.framework) æ¶æ„ä¸€è‡´
4. é›¶é…ç½®ï¼šç”¨æˆ·æ— éœ€å®‰è£… ROCm toolkit

**ç›®å½•ç»“æ„**ï¼š
```
src/hip_kernels/
â”œâ”€â”€ mod.rs              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ hsa_runtime.rs      # HSA Runtime åŠ¨æ€åŠ è½½ï¼ˆlibhsa-runtime64.soï¼‰
â”œâ”€â”€ hsa_flash_attn.rs   # HSA Flash Attention Kernel
â”œâ”€â”€ hsa_paged_attn.rs   # HSA Paged Attention Kernel
â””â”€â”€ kernels/
    â”œâ”€â”€ flash_attention.hsaco   # é¢„ç¼–è¯‘ HSACOï¼ˆinclude_bytes!ï¼‰
    â””â”€â”€ paged_attention.hsaco   # é¢„ç¼–è¯‘ HSACOï¼ˆinclude_bytes!ï¼‰
```

**å…¬å¼€æ¥å£**ï¼š

| æ¨¡å— | å¯¼å‡º | è¯´æ˜ |
|------|------|------|
| `hsa_runtime` | `get_hsa_lib`, `is_hsa_available`, `HsaLib`, `GpuAgent`, `find_gpu_agents` | HSA è¿è¡Œæ—¶æŠ½è±¡ |
| `hsa_flash_attn` | `HsaFlashAttentionKernel`, `HsaBuffer`, `HsaQueueWrapper` | Flash Attention |
| `hsa_paged_attn` | `HsaPagedAttentionKernel` | Paged Attention |
| ä¾¿æ·å‡½æ•° | `is_amd_gpu_available()` | æ£€æµ‹ AMD GPU å¯ç”¨æ€§ |

**HSA Runtime æ ¸å¿ƒæŠ½è±¡**ï¼š

| ç±»å‹ | è¯´æ˜ |
|------|------|
| `HsaLib` | HSA Library åŠ¨æ€åŠ è½½ï¼ˆé€šè¿‡ libloadingï¼‰ |
| `GpuAgent` | GPU Agent æŠ½è±¡ï¼ˆhandle + nameï¼‰ |
| `HsaKernelModule` | HSA Kernel Moduleï¼ˆHSACO åŠ è½½ï¼‰ |
| `HsaQueue` | HSA å‘½ä»¤é˜Ÿåˆ— |
| `HsaSignal` | HSA åŒæ­¥ä¿¡å· |

| å‡½æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|----------|------|
| `is_hsa_available()` | bool | æ£€æµ‹ HSA æ˜¯å¦å¯ç”¨ |
| `get_hsa_lib()` | Result\<HsaLib\> | è·å– HSA åº“å®ä¾‹ |
| `find_gpu_agents()` | Vec\<GpuAgent\> | æŸ¥æ‰¾æ‰€æœ‰ GPU Agent |

**Kernel å®ç°æ¨¡å¼**ï¼š

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| é¢„ç¼–è¯‘ HSACO | é€šè¿‡ `include_bytes!` åµŒå…¥ |
| `HsaFlashAttentionKernel` | åŒ…å« agent + module_f32 + module_f16 |

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `new(agent)` | ä» GpuAgent åˆ›å»º Kernel å®ä¾‹ |
| `forward_f32(...)` | f32 å‰å‘è®¡ç®— |
| `forward_f16(...)` | f16 å‰å‘è®¡ç®— |

---

## ARCH-METAL-001: Metal åç«¯ï¼ˆé›¶é…ç½® + metallib ä¸­é—´æ€åŠ è½½ï¼‰

**å†³ç­–**ï¼šç§»é™¤ `metal-precompiled` featureï¼Œmetallib åµŒå…¥ä¸ºé»˜è®¤è¡Œä¸ºï¼Œæ— ç¯å¢ƒå˜é‡é…ç½®

**è®¾è®¡åŸåˆ™**ï¼ˆéµå¾ª ARCH-LOAD-001ï¼‰ï¼š
- ç”¨æˆ·é›¶é…ç½®ï¼Œå³æ’å³ç”¨
- åªä»åµŒå…¥çš„ metallib ä¸­é—´æ€åŠ è½½
- ğŸš¨ ç¦æ­¢ä» .metal æºç è¿è¡Œæ—¶ç¼–è¯‘

**åŠ è½½æµç¨‹**ï¼š

| æ­¥éª¤ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| 1 | è¯»å–åµŒå…¥çš„ metallib | `include_bytes!("kernels/flash_attention.metallib")` |
| 2 | æ£€æŸ¥æœ‰æ•ˆæ€§ | å¦‚æœä¸ºç©ºï¼Œè¿”å› `KernelNotFound` é”™è¯¯ |
| 3 | åŠ è½½åˆ° Metal Device | `device.new_library_with_data()` |
| 4 | ğŸš¨ æ— å›é€€ | ç¦æ­¢ä» .metal æºç ç¼–è¯‘ï¼ |

**Metal ç›®å½•ç»“æ„**ï¼š
```
src/metal_kernels/
â”œâ”€â”€ mod.rs
â”œâ”€â”€ metal_runtime.rs        # Metal Runtime æŠ½è±¡
â”œâ”€â”€ metallib_loader.rs      # MetallibCollection ä¸­é—´æ€åŠ è½½
â”œâ”€â”€ flash_attn.rs           # Flash Attention
â”œâ”€â”€ paged_attn.rs           # Paged Attention
â””â”€â”€ kernels/
    â”œâ”€â”€ flash_attention.metallib  # é¢„ç¼–è¯‘ä¸­é—´æ€ï¼ˆCI ç”Ÿæˆï¼Œinclude_bytes!ï¼‰
    â””â”€â”€ paged_attention.metallib  # é¢„ç¼–è¯‘ä¸­é—´æ€ï¼ˆCI ç”Ÿæˆï¼Œinclude_bytes!ï¼‰
```

---

## ARCH-DISPATCH-001: åç«¯æ´¾å‘ï¼ˆå·²ç®€åŒ–ï¼‰

> âš ï¸ **å·²ç®€åŒ–**ï¼šå‚è§ ARCH-BACKEND-001ï¼Œä½¿ç”¨ `Arc<dyn Backend>` æ›¿ä»£ enum æ´¾å‘ã€‚
>
> å¯åŠ¨æ—¶é€‰ä¸€æ¬¡åç«¯ï¼Œè¿”å› `Arc<dyn Backend>`ï¼Œä¹‹åç›´æ¥è°ƒç”¨æ–¹æ³•ã€‚
> çƒ­è·¯å¾„å†…æ— ä»»ä½•é¢å¤–åˆ†å‘å¼€é”€ã€‚

---

## ARCH-RUNTIME-001: è¿è¡Œæ—¶åç«¯æ£€æµ‹ï¼ˆå…¨è‡ªåŠ¨ï¼‰

**è®¾è®¡åŸåˆ™**ï¼š
- **å®Œå…¨è‡ªåŠ¨åŒ–**ï¼šæ— éœ€ç”¨æˆ·é…ç½®ï¼Œè‡ªåŠ¨æ£€æµ‹æœ€ä¼˜åç«¯
- **é›¶é…ç½®**ï¼šä¸è¯»å–ä»»ä½•ç¯å¢ƒå˜é‡
- **ç¡®å®šæ€§é¡ºåº**ï¼šæŒ‰æ€§èƒ½ä¼˜å…ˆçº§æ£€æµ‹ï¼ˆCUDA > ROCm > Metal > WGPU > CPUï¼‰
- **ä¸€æ¬¡æ£€æµ‹**ï¼šè¿”å› `Arc<dyn Backend>`ï¼Œåç»­ç›´æ¥ä½¿ç”¨

**æ£€æµ‹æµç¨‹**ï¼ˆå‚è§ ARCH-BACKEND-001ï¼‰ï¼š

```rust
pub fn auto_select_backend() -> Arc<dyn Backend> {
    if cuda_available() { return Arc::new(CudaBackend::new()); }
    if rocm_available() { return Arc::new(RocmBackend::new()); }
    if metal_available() { return Arc::new(MetalBackend::new()); }
    if wgpu_available() { return Arc::new(WgpuBackend::new()); }
    Arc::new(CpuBackend::new())
}
```

**æ£€æµ‹é¡ºåºè¡¨**ï¼š

| ä¼˜å…ˆçº§ | æ£€æµ‹ | è¿”å› |
|--------|------|------|
| 1 | CUDA | `Arc<CudaBackend>` |
| 2 | ROCm | `Arc<RocmBackend>` |
| 3 | Metal | `Arc<MetalBackend>` |
| 4 | WGPU | `Arc<WgpuBackend>` |
| 5 | CPU | `Arc<CpuBackend>` |

**ğŸš¨ è®¾è®¡çº¦æŸ**ï¼š
- âŒ **ç¦æ­¢ `GLLM_BACKEND` ç¯å¢ƒå˜é‡**ï¼šç”¨æˆ·ä¸åº”æ‰‹åŠ¨æŒ‡å®šåç«¯
- âœ… **è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜**ï¼šå§‹ç»ˆé€‰æ‹©æ£€æµ‹åˆ°çš„æœ€é«˜æ€§èƒ½åç«¯

---

## ARCH-FLOW-001: æ‰§è¡Œæµç¨‹ï¼ˆğŸš¨ å…¨æµç¨‹å•åç«¯ï¼‰

> âš ï¸ **æ ¸å¿ƒåŸåˆ™**ï¼šé€‰å®šåç«¯åï¼Œæ•´ä¸ªæ¨ç†æµç¨‹åœ¨åŒä¸€åç«¯æ‰§è¡Œï¼Œç¦æ­¢æ··åˆï¼

**æ ¸å¿ƒæµç¨‹**ï¼š

```
ç¨‹åºå¯åŠ¨ â†’ auto_select_backend() â†’ Arc<dyn Backend> â†’ å…¨æµç¨‹ä½¿ç”¨åŒä¸€ backend
```

| é˜¶æ®µ | æ“ä½œ |
|------|------|
| 1. åç«¯é€‰æ‹© | `auto_select_backend()` è¿”å› `Arc<dyn Backend>` |
| 2. æ•°æ®ä¸Šä¼  | `backend.upload(host_data, &mut gpu_tensor)` |
| 3. **å…¨æµç¨‹è®¡ç®—** | `backend.flash_attention(...)` â†’ `backend.rms_norm(...)` â†’ ... |
| 4. ç»“æœä¸‹è½½ | `backend.readback(&gpu_tensor, &mut host_data)` |

**ğŸš¨ ç¦æ­¢çš„æ¨¡å¼**ï¼š

```rust
// âŒ é”™è¯¯ï¼šGPU/CPU æ··åˆæ‰§è¡Œ
let gpu_out = backend.flash_attention(gpu_q, gpu_k, gpu_v);
backend.readback(&gpu_out, &mut host_data);  // ä¸‹è½½åˆ° CPU
let cpu_out = cpu_rms_norm(&host_data);       // CPU è®¡ç®—
backend.upload(&cpu_out, &mut gpu_tensor);    // å†ä¸Šä¼ 
// â†‘ æ¯æ¬¡ readback/upload éƒ½æ˜¯æ¯«ç§’çº§å¼€é”€ï¼

// âœ… æ­£ç¡®ï¼šå…¨æµç¨‹ GPU
let gpu_out = backend.flash_attention(gpu_q, gpu_k, gpu_v);
backend.rms_norm(&gpu_out, &mut gpu_norm);    // GPU ç»§ç»­
backend.linear_forward(&gpu_norm, ...);        // GPU ç»§ç»­
backend.readback(&final_gpu, &mut host_result); // æœ€åæ‰ä¸‹è½½
```

**åˆ†å¸ƒå¼æ‰§è¡Œ**ï¼š
- âš ï¸ **æœªå®ç°**ï¼šRing Attention ç­‰åˆ†å¸ƒå¼ç®—å­æš‚æœªå®ç°

---

## ARCH-SLICE-001: åŸç”Ÿåˆ‡ç‰‡æ¥å£

**é—®é¢˜**: Tensor æŠ½è±¡å±‚ä¼šå¼•å…¥é¢å¤–å¼€é”€

**å†³ç­–**: ç®—å­æ¥å£ä½¿ç”¨åŸç”Ÿåˆ‡ç‰‡

#### flash_attention æ¥å£

| å‚æ•° | ç±»å‹ | å¸ƒå±€ |
|------|------|------|
| q | slice\<T\> | [batch, heads, seq_len, head_dim] |
| k | slice\<T\> | [batch, heads, seq_len, head_dim] |
| v | slice\<T\> | [batch, heads, seq_len, head_dim] |
| config | FlashAttentionConfig | é…ç½®å‚æ•° |
| è¿”å›å€¼ | Result\<Vec\<T\>\> | è¾“å‡ºæ•°æ® |

#### è°ƒç”¨æµç¨‹

| æ­¥éª¤ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| 1 | Tensor â†’ Slice | è°ƒç”¨æ–¹ä» Tensor æå–åˆ‡ç‰‡ |
| 2 | æ‰§è¡Œ Kernel | ä¼ å…¥åŸç”Ÿåˆ‡ç‰‡è°ƒç”¨ç®—å­ |
| 3 | Slice â†’ Tensor | è°ƒç”¨æ–¹ä»è¿”å›çš„åˆ‡ç‰‡æ„é€  Tensor |

**è®¾è®¡åŸåˆ™**ï¼šè°ƒç”¨æ–¹è´Ÿè´£å†…å­˜ç®¡ç†ï¼Œgllm-kernels ä¸ä¾èµ–ä»»ä½• Tensor æŠ½è±¡

---

## é«˜çº§ç®—å­æ¶æ„ï¼ˆ2025-2026 æŠ€æœ¯å‡çº§ï¼‰

> **å®ç°çŠ¶æ€æ€»è§ˆ**ï¼ˆ2026-01 æ›´æ–°ï¼‰ï¼š
>
> | ID | ç®—å­ | çŠ¶æ€ | ä»£ç ä½ç½® | ä»£ç è¡Œæ•° |
> |-----|------|------|----------|----------|
> | ARCH-OP-007 | StreamingLLM | âŒ æœªå®ç° | - | - |
> | ARCH-OP-008 | EAGLE-3 | âœ… å·²å®ç° | `ops/eagle3/` | ~976 è¡Œ |
> | ARCH-OP-009 | SpecEE/LayerSkip | âœ… å·²å®ç° | `ops/spec_ee/` | ~937 è¡Œ |
> | ARCH-OP-010 | Flash Tree-attention | âœ… å·²å®ç° | `ops/flash_tree_attn.rs` | ~1122 è¡Œ |
> | ARCH-OP-011 | INT2/EvicPress | âœ… å·²å®ç° | `ops/int2_quantizer.rs`, `ops/evic_press.rs` | ~1333 è¡Œ |
> | ARCH-OP-012 | Infinite Retrieval | âŒ æœªå®ç° | - | - |
> | ARCH-OP-013 | Medusa | âœ… å·²å®ç° | `ops/medusa/` | ~1008 è¡Œ |
> | ARCH-OP-014 | Prompt Cache | âœ… å·²å®ç° | `ops/prompt_cache.rs` | ~1333 è¡Œ |
> | ARCH-OP-015 | Chunked Prefill | âœ… å·²å®ç° | `ops/chunked_prefill.rs` | ~1048 è¡Œ |

### ARCH-OP-007: StreamingLLM / Attention Sink

> **âš ï¸ å®ç°çŠ¶æ€**ï¼šâŒ æœªå®ç°ï¼ˆè®¾è®¡æ–‡æ¡£ï¼‰

**è®¾è®¡ç›®æ ‡**: æ”¯æŒæ— é™é•¿åº¦åºåˆ—æ¨ç†ï¼Œé€šè¿‡ Attention Sink æœºåˆ¶ä¿æŒç”Ÿæˆè´¨é‡

**æ ¸å¿ƒåŸç†**:
- åˆå§‹ tokenï¼ˆAttention Sinkï¼‰èšé›†å¤§é‡æ³¨æ„åŠ›æƒé‡ï¼Œå¯¹ç”Ÿæˆè´¨é‡è‡³å…³é‡è¦
- æ»‘åŠ¨çª—å£ç»´æŠ¤æœ€è¿‘ L ä¸ª token çš„ KV
- å†…å­˜å¤æ‚åº¦ä» O(T) é™è‡³ O(N+L) å¸¸æ•°

**æ¶æ„è®¾è®¡**:

```
KV Cache å¸ƒå±€:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sink Tokens   â”‚         Sliding Window              â”‚
â”‚   (å›ºå®š N ä¸ª)   â”‚      (æ»‘åŠ¨ï¼Œæœ€è¿‘ L ä¸ª)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Position: 0..N          Position: T-L..T

Attention è®¡ç®—:
Q_current Ã— [K_sink | K_window]^T â†’ Attention Scores
```

**StreamingKVCache ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| sink_k | Buffer | Sink token çš„ Key: [num_layers, sink_size, num_heads, head_dim] |
| sink_v | Buffer | Sink token çš„ Value: [num_layers, sink_size, num_heads, head_dim] |
| window_k | CircularBuffer | æ»‘åŠ¨çª—å£ Key: [num_layers, window_size, num_heads, head_dim] |
| window_v | CircularBuffer | æ»‘åŠ¨çª—å£ Value: [num_layers, window_size, num_heads, head_dim] |
| sink_size | usize | Sink token æ•°é‡ï¼ˆé»˜è®¤ 4ï¼‰ |
| window_size | usize | æ»‘åŠ¨çª—å£å¤§å°ï¼ˆé»˜è®¤ 512 æˆ– 1024ï¼‰ |
| current_pos | usize | å½“å‰å†™å…¥ä½ç½® |

**CircularBuffer ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Buffer | åº•å±‚å­˜å‚¨ |
| head | usize | ç¯å½¢ç¼“å†²åŒºå¤´æŒ‡é’ˆ |
| capacity | usize | ç¯å½¢ç¼“å†²åŒºå®¹é‡ |

**æ¥å£è®¾è®¡**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `new` | config: StreamingConfig | Self | åˆ›å»º StreamingKVCache |
| `append` | k: Buffer, v: Buffer, pos: usize | () | è¿½åŠ  KVï¼ˆè‡ªåŠ¨å¤„ç† sink/window åˆ†é…ï¼‰ |
| `get_attention_kv` | - | (Buffer, Buffer) | è·å–ç”¨äº Attention è®¡ç®—çš„ KVï¼ˆsink + windowï¼‰ |
| `clear_window` | - | () | æ¸…ç©ºæ»‘åŠ¨çª—å£ï¼ˆä¿ç•™ sinkï¼‰ |

**StreamingConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| sink_size | usize | 4 | Attention Sink token æ•°é‡ |
| window_size | usize | 512 | æ»‘åŠ¨çª—å£å¤§å° |
| num_layers | usize | - | æ¨¡å‹å±‚æ•° |
| num_heads | usize | - | æ³¨æ„åŠ›å¤´æ•° |
| head_dim | usize | - | å¤´ç»´åº¦ |

**ä¸ç°æœ‰ç»„ä»¶é›†æˆ**:

| é›†æˆç‚¹ | æ–¹å¼ |
|--------|------|
| PagedAttention | window_k/v å¯ä½¿ç”¨ Paged å­˜å‚¨ |
| KV Cache å‹ç¼© | sink ä¿æŒé«˜ç²¾åº¦ï¼Œwindow å¯ä½¿ç”¨ Low-rank/VQ |
| Ring Attention | æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹ç»´æŠ¤ StreamingKVCache |

---

### ARCH-OP-008: EAGLE-3 è‡ªé€‚åº”è‰ç¨¿é•¿åº¦

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - ä»£ç ä½ç½®ï¼š`src/ops/eagle3/`
> - æ ¸å¿ƒæ¨¡å—ï¼š`decoder.rs`(343è¡Œ), `predictor.rs`(136è¡Œ), `scheduler.rs`(110è¡Œ)
> - å¯¼å‡ºï¼š`AdaptiveDraftConfig`, `ConfidencePredictor`, `Eagle3Decoder`, `LengthScheduler`

**è®¾è®¡ç›®æ ‡**: å‡çº§æŠ•æœºè§£ç ï¼ŒåŸºäºç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´è‰ç¨¿é•¿åº¦ï¼Œå®ç° 2-6x åŠ é€Ÿ

**åŸºäºè®ºæ–‡**: [EAGLE-3](https://arxiv.org/abs/2504.08850) (NeurIPS'25)

**æ ¸å¿ƒåŸç†**:
- åŸºäºéšè—çŠ¶æ€é¢„æµ‹ target model æ¥å—æ¦‚ç‡
- ç½®ä¿¡åº¦ä½æ—¶æå‰ç»ˆæ­¢è‰ç¨¿ç”Ÿæˆï¼Œé¿å…æµªè´¹è®¡ç®—
- å­¦ä¹ æœ€ä¼˜è‰ç¨¿é•¿åº¦åˆ†å¸ƒï¼Œè‡ªé€‚åº”ä¸åŒè¾“å…¥
- **å¤šå±‚ç‰¹å¾èåˆ**ï¼šèåˆå¤šä¸ª Transformer å±‚çš„éšè—çŠ¶æ€ï¼ˆvs EAGLE-2 å•å±‚ï¼‰
- **Token çº§é¢„æµ‹**ï¼šä»åºåˆ—çº§é¢„æµ‹å‡çº§ä¸º token çº§ç½®ä¿¡åº¦é¢„æµ‹
- **Training-time Test**ï¼šè®­ç»ƒæ—¶æ¨¡æ‹Ÿæµ‹è¯•åˆ†å¸ƒï¼Œæå‡æ³›åŒ–

**æ¶æ„è®¾è®¡**:

```
Draft Generation Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  Hidden State h_t                                           â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚ Confidence Head â”‚ â†’ p_accept = Ïƒ(W_c Â· h_t)              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  p_accept < threshold? â”€â”€Yesâ”€â”€â†’ Stop Draft (Early Exit)     â”‚
â”‚       â”‚                                                     â”‚
â”‚       No                                                    â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  Generate Next Draft Token                                  â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  draft_length < max_length? â”€â”€Yesâ”€â”€â†’ Continue               â”‚
â”‚       â”‚                                                     â”‚
â”‚       No                                                    â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  Submit to Verification                                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**AdaptiveDraftConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| min_draft_length | usize | 1 | æœ€å°è‰ç¨¿é•¿åº¦ |
| max_draft_length | usize | 8 | æœ€å¤§è‰ç¨¿é•¿åº¦ |
| confidence_threshold | f32 | 0.5 | ç½®ä¿¡åº¦é˜ˆå€¼ |
| fallback_length | usize | 3 | éªŒè¯å¤±è´¥åçš„å›é€€é•¿åº¦ |
| enable_length_scheduler | bool | true | æ˜¯å¦å¯ç”¨é•¿åº¦è°ƒåº¦å™¨ |

**ConfidencePredictor ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| weight | Buffer | çº¿æ€§å±‚æƒé‡: [hidden_dim, 1] |
| bias | f32 | åç½®é¡¹ |

**ConfidencePredictor æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `predict` | hidden_state: Buffer | f32 | é¢„æµ‹æ¥å—æ¦‚ç‡ï¼ˆsigmoid è¾“å‡ºï¼‰ |

**LengthScheduler ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| length_distribution | Vec\<f32\> | å„é•¿åº¦çš„å†å²æ¥å—ç‡ |
| ema_alpha | f32 | æŒ‡æ•°ç§»åŠ¨å¹³å‡ç³»æ•° |
| sample_count | Vec\<usize\> | å„é•¿åº¦çš„é‡‡æ ·æ¬¡æ•° |

**LengthScheduler æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `suggest_length` | - | usize | åŸºäºå†å²å»ºè®®è‰ç¨¿é•¿åº¦ |
| `update` | length: usize, accepted: usize | () | æ›´æ–°ç»Ÿè®¡ï¼ˆæ¥å—äº†å¤šå°‘ tokenï¼‰ |

**å›é€€ç­–ç•¥**:

| éªŒè¯ç»“æœ | ä¸‹æ¬¡è‰ç¨¿é•¿åº¦ |
|----------|-------------|
| å…¨éƒ¨æ¥å— | min(current + 1, max) |
| éƒ¨åˆ†æ¥å— | max(accepted_count, min) |
| å…¨éƒ¨æ‹’ç» | fallback_length |

---

### ARCH-OP-009: SpecEE / LayerSkip æ—©é€€å‡ºæ¨æµ‹

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - ä»£ç ä½ç½®ï¼š`src/ops/spec_ee/`
> - æ ¸å¿ƒæ¨¡å—ï¼š`engine.rs`(205è¡Œ), `head.rs`(186è¡Œ), `cache.rs`(178è¡Œ)
> - å¯¼å‡ºï¼š`SpecEEConfig`, `SpecEEEngine`, `EarlyExitHead`, `LayerDropoutSchedule`, `SharedActivations`

**è®¾è®¡ç›®æ ‡**: æ•´åˆæ—©é€€å‡ºæœºåˆ¶ä¸æŠ•æœºè§£ç ï¼ŒåŒä¸€æ¨¡å‹å†…éƒ¨å®Œæˆè‰ç¨¿-éªŒè¯å¾ªç¯ï¼Œç›®æ ‡ 2.25-4x å»¶è¿Ÿé™ä½

**åŸºäºè®ºæ–‡**:
- [SpecEE](https://dl.acm.org/doi/10.1145/3695053.3730996) (ISCA'25) - 2.25-2.43x åŠ é€Ÿ
- [LayerSkip](https://arxiv.org/abs/2404.16710) (ACL'24) - Self-Speculative Decoding

**æ ¸å¿ƒåŸç†**:
- æ¯å±‚è®¾ç½® Early Exit Headï¼Œè®¡ç®—ç½®ä¿¡åº¦
- é«˜ç½®ä¿¡åº¦æ—¶ä»æ—©æœŸå±‚é€€å‡ºä½œä¸º"è‰ç¨¿"
- å®Œæ•´å‰å‘ä½œä¸º"éªŒè¯"ï¼Œæ¥å—æˆ–æ‹’ç»æ—©é€€å‡ºç»“æœ
- **Layer Dropout è®­ç»ƒ**ï¼šè®­ç»ƒæ—¶ä½å±‚ä½ dropout rateï¼Œé«˜å±‚é«˜ dropout rateï¼Œå¢å¼ºæ—©æœŸå±‚ç‹¬ç«‹æ€§
- **å…±äº«æ¿€æ´»ä¼˜åŒ–**ï¼šè‰ç¨¿å’ŒéªŒè¯é˜¶æ®µå…±äº«æ—©æœŸå±‚çš„è®¡ç®—å’Œ KV Cache

**æ¶æ„è®¾è®¡**:

```
Self-Speculation Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 0  â”€â”€â†’  Layer 1  â”€â”€â†’  ...  â”€â”€â†’  Layer N-1  â”€â”€â†’ Final â”‚
â”‚     â”‚            â”‚                         â”‚                â”‚
â”‚     â–¼            â–¼                         â–¼                â”‚
â”‚  [EE Head]   [EE Head]               [EE Head]              â”‚
â”‚     â”‚            â”‚                         â”‚                â”‚
â”‚     â–¼            â–¼                         â–¼                â”‚
â”‚  conf_0       conf_1                   conf_N-1             â”‚
â”‚     â”‚            â”‚                         â”‚                â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚              Max confidence layer = exit_layer              â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚              conf[exit_layer] > threshold?                  â”‚
â”‚                   â”‚                  â”‚                      â”‚
â”‚                  Yes                 No                     â”‚
â”‚                   â”‚                  â”‚                      â”‚
â”‚                   â–¼                  â–¼                      â”‚
â”‚           Use Early Exit      Continue Full Forward        â”‚
â”‚           (Draft Output)       (Verification)              â”‚
â”‚                   â”‚                  â”‚                      â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚              Verify: EE output == Full output?              â”‚
â”‚                   â”‚                  â”‚                      â”‚
â”‚                  Yes                 No                     â”‚
â”‚                   â”‚                  â”‚                      â”‚
â”‚                   â–¼                  â–¼                      â”‚
â”‚               Accept EE         Reject, use Full           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**EarlyExitHead ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| lm_head | Buffer | è¯­è¨€æ¨¡å‹å¤´: [hidden_dim, vocab_size] |
| confidence_head | Buffer | ç½®ä¿¡åº¦å¤´: [hidden_dim, 1] |
| layer_idx | usize | æ‰€å±å±‚ç´¢å¼• |

**EarlyExitHead æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `forward` | hidden: Buffer | (logits: Buffer, confidence: f32) | è¾“å‡º logits å’Œç½®ä¿¡åº¦ |

**SpecEEConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| exit_layers | Vec\<usize\> | [6, 12, 18] | é…ç½®æ—©é€€å‡ºçš„å±‚ |
| confidence_threshold | f32 | 0.8 | æ—©é€€å‡ºç½®ä¿¡åº¦é˜ˆå€¼ |
| min_exit_layer | usize | 6 | æœ€å°é€€å‡ºå±‚ï¼ˆä¿è¯è´¨é‡ï¼‰ |
| speculation_depth | usize | 4 | è‡ªæ¨æµ‹æ·±åº¦ |
| enable_layer_dropout | bool | true | å¯ç”¨ Layer Dropout è®­ç»ƒæ¨¡å¼ |
| layer_dropout_rate | fn(usize)->f32 | linear(0.1, 0.5) | å±‚çº§ dropout rate å‡½æ•° |
| share_activations | bool | true | å¯ç”¨è‰ç¨¿-éªŒè¯æ¿€æ´»å…±äº« |

**SpecEEEngine ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| early_exit_heads | Vec\<EarlyExitHead\> | å„å±‚çš„æ—©é€€å‡ºå¤´ |
| config | SpecEEConfig | é…ç½® |
| stats | SpecEEStats | è¿è¡Œæ—¶ç»Ÿè®¡ |

**SpecEEStats ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| exit_layer_counts | Vec\<usize\> | å„å±‚é€€å‡ºæ¬¡æ•° |
| acceptance_rate | f32 | æ—©é€€å‡ºæ¥å—ç‡ |
| avg_exit_layer | f32 | å¹³å‡é€€å‡ºå±‚ |

---

### ARCH-OP-010: DeFT / Talon Flash Tree-attention

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - ä»£ç ä½ç½®ï¼š`src/ops/flash_tree_attn.rs`ï¼ˆ1122 è¡Œï¼‰
> - å¯¼å‡ºï¼š`FlashTreeAttention`, `TokenTree`, `TreeMask`, `TalonController`, `BatchTreeConfig`, `TalonConfig`

**è®¾è®¡ç›®æ ‡**: ä¼˜åŒ–æ ‘éªŒè¯ç®—æ³•ï¼Œå®ç° O(n+m) å¤æ‚åº¦çš„ Flash Tree-attentionï¼Œæ‰¹å¤„ç†åœºæ™¯åŠ é€Ÿ 2-4x

**åŸºäºè®ºæ–‡**:
- [DeFT](https://arxiv.org/abs/2404.00242) (ICLR'25) - Flash Tree-attention
- [Talon](https://arxiv.org/abs/2501.08076) (ICLR'26) - ç½®ä¿¡åº¦è‡ªé€‚åº” Token Tree
- [SEQUOIA](https://arxiv.org/abs/2402.12374) - åŠ¨æ€æ ‘ç»“æ„ä¼˜åŒ–

**æ ¸å¿ƒåŸç†**:
- æŠ•æœºè§£ç ç”Ÿæˆ token treeï¼ˆå¤šä¸ªå€™é€‰è·¯å¾„ï¼‰
- ä¼ ç»Ÿæ–¹æ³•é€è·¯å¾„éªŒè¯ï¼ŒO(n*m) å¤æ‚åº¦
- DeFT é€šè¿‡æ ‘ç»“æ„åˆ†è§£ï¼Œä¸€æ¬¡ Attention è®¡ç®—æ‰€æœ‰è·¯å¾„
- **DeFT-Flatten**ï¼šå‡åŒ€åˆ†å‰²æ ‘ç»“æ„åˆ° GPU SMsï¼Œæœ€å¤§åŒ–å¹¶è¡Œåº¦
- **DeFT-Node**ï¼šèŠ‚ç‚¹çº§å¹¶è¡Œï¼Œæ¯ä¸ª SM å¤„ç†å¤šä¸ªèŠ‚ç‚¹
- **Talon ç½®ä¿¡åº¦è‡ªé€‚åº”**ï¼šæ ¹æ®å†å²æ¥å—ç‡åŠ¨æ€è°ƒæ•´æ ‘ç»“æ„
- **Traversal Verification**ï¼šåºåˆ—çº§éªŒè¯æ›¿ä»£ token çº§ï¼Œå‡å°‘éªŒè¯å¼€é”€

**æ¶æ„è®¾è®¡**:

```
Token Tree Structure:
                 [root]
                /  |   \
            [a]   [b]   [c]
           / |     |    / \
        [d] [e]   [f] [g] [h]

Linearized Sequence (DFS order):
[root, a, d, e, b, f, c, g, h]

Tree Mask (Causal + Tree Structure):
        root  a  d  e  b  f  c  g  h
root  [  1   0  0  0  0  0  0  0  0 ]
a     [  1   1  0  0  0  0  0  0  0 ]
d     [  1   1  1  0  0  0  0  0  0 ]
e     [  1   1  0  1  0  0  0  0  0 ]
b     [  1   0  0  0  1  0  0  0  0 ]
f     [  1   0  0  0  1  1  0  0  0 ]
c     [  1   0  0  0  0  0  1  0  0 ]
g     [  1   0  0  0  0  0  1  1  0 ]
h     [  1   0  0  0  0  0  1  0  1 ]
```

**TokenTree ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| tokens | Vec\<TokenId\> | DFS çº¿æ€§åŒ–çš„ token åºåˆ— |
| parent_indices | Vec\<i32\> | æ¯ä¸ªèŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ç´¢å¼•ï¼ˆroot=-1ï¼‰ |
| depth | Vec\<usize\> | æ¯ä¸ªèŠ‚ç‚¹çš„æ·±åº¦ |
| num_nodes | usize | èŠ‚ç‚¹æ€»æ•° |

**TreeMask ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| mask_data | Buffer | å‹ç¼©çš„æ©ç æ•°æ®ï¼ˆbit-packedï¼‰ |
| num_nodes | usize | èŠ‚ç‚¹æ•° |

**TreeMask ç”Ÿæˆè§„åˆ™**:

| è§„åˆ™ | å…¬å¼ |
|------|------|
| mask[i][j] = 1 å½“ä¸”ä»…å½“ | j æ˜¯ i çš„ç¥–å…ˆï¼ˆå«è‡ªèº«ï¼‰ |

**FlashTreeAttention ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| flash_attn_kernel | FlashAttentionKernel | åº•å±‚ Flash Attention |
| tree_mask_builder | TreeMaskBuilder | æ ‘æ©ç æ„å»ºå™¨ |

**FlashTreeAttention æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `forward` | q: Buffer, k: Buffer, v: Buffer, tree: TokenTree | Buffer | æ ‘æ³¨æ„åŠ›å‰å‘ |
| `batch_forward` | batch_trees: Vec\<TokenTree\> | Vec\<Buffer\> | æ‰¹é‡æ ‘æ³¨æ„åŠ› |

**BatchTreeConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| max_batch_size | usize | 8 | æœ€å¤§æ‰¹å¤§å° |
| max_tree_depth | usize | 8 | æœ€å¤§æ ‘æ·±åº¦ |
| max_nodes_per_tree | usize | 64 | æ¯æ£µæ ‘æœ€å¤§èŠ‚ç‚¹æ•° |
| partition_strategy | PartitionStrategy | DeFTFlatten | æ ‘åˆ†å‰²ç­–ç•¥ï¼ˆFlatten/Nodeï¼‰ |
| enable_talon | bool | true | å¯ç”¨ Talon ç½®ä¿¡åº¦è‡ªé€‚åº” |
| traversal_verification | bool | true | å¯ç”¨åºåˆ—çº§ Traversal Verification |

**TalonConfig ç»“æ„ï¼ˆç½®ä¿¡åº¦è‡ªé€‚åº”æ ‘ï¼‰**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| acceptance_history_size | usize | 100 | å†å²æ¥å—ç‡çª—å£å¤§å° |
| tree_expansion_threshold | f32 | 0.8 | æ¥å—ç‡é«˜äºæ­¤å€¼æ—¶æ‰©å±•æ ‘ |
| tree_shrink_threshold | f32 | 0.3 | æ¥å—ç‡ä½äºæ­¤å€¼æ—¶æ”¶ç¼©æ ‘ |
| min_branches | usize | 2 | æœ€å°åˆ†æ”¯æ•° |
| max_branches | usize | 8 | æœ€å¤§åˆ†æ”¯æ•° |

**å¤æ‚åº¦åˆ†æ**:

| æ–¹æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |
|------|-----------|-----------|
| é€è·¯å¾„éªŒè¯ | O(n * m * d) | O(n + d) |
| DeFT Flash Tree | O(n + m) | O(m) |

> n = prompt length, m = tree nodes, d = tree depth

---

### ARCH-OP-011: PM-KVQ / EvicPress INT2 é‡åŒ–ä¸é©±é€

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - INT2 é‡åŒ–ï¼š`src/ops/int2_quantizer.rs`ï¼ˆ501 è¡Œï¼‰
> - EvicPress é©±é€ï¼š`src/ops/evic_press.rs`ï¼ˆ832 è¡Œï¼‰
> - å¯¼å‡ºï¼š`Int2Quantizer`, `Int2PackedBuffer`, `ProgressiveKVCache`, `EvicPressConfig`, `TokenImportance`

**è®¾è®¡ç›®æ ‡**: æç«¯ KV Cache é‡åŒ–ï¼Œæ”¯æŒ INT2 ç²¾åº¦ï¼Œç»“åˆæ™ºèƒ½é©±é€ç­–ç•¥ï¼Œé¢å¤–èŠ‚çœ 25-75% å†…å­˜

**åŸºäºè®ºæ–‡**:
- [PM-KVQ](https://arxiv.org/abs/2406.02069) - Progressive Mixed-precision Quantization
- [EvicPress](https://arxiv.org/abs/2503.00909) - è”åˆå‹ç¼©ä¸é©±é€ç­–ç•¥
- [MiniKV](https://arxiv.org/abs/2411.14625) - 2-bit æç«¯é‡åŒ–

**æ ¸å¿ƒåŸç†**:
- 2-bit é‡åŒ–ï¼šæ¯ä¸ªå…ƒç´ åªéœ€ 2 bitsï¼ˆ4 ä¸ªé‡åŒ–çº§åˆ«ï¼‰
- æ¸è¿›å¼é‡åŒ–ï¼šçƒ­ token ä¿æŒé«˜ç²¾åº¦ï¼Œå†· token é€æ¸é™çº§
- ä¸ç°æœ‰å‹ç¼©æ–¹æ¡ˆï¼ˆLow-rank, VQï¼‰å…¼å®¹
- **EvicPress è”åˆç­–ç•¥**ï¼šå‹ç¼©å’Œé©±é€ååŒå†³ç­–ï¼Œè€Œéç‹¬ç«‹æ“ä½œ
- **é‡è¦æ€§è¯„åˆ†**ï¼šç»“åˆæ³¨æ„åŠ›åˆ†æ•°ã€ä½ç½®è¡°å‡ã€è¯­ä¹‰é‡è¦æ€§

**æ¶æ„è®¾è®¡**:

```
Progressive Quantization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KV Cache å¸ƒå±€                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Hot Zone (FP16)  â”‚  Warm Zone (INT8)  â”‚  Cold Zone (INT2) â”‚
â”‚  [æœ€è¿‘ 64 tokens] â”‚  [64-256 tokens]   â”‚  [>256 tokens]    â”‚
â”‚   å†…å­˜: 4B/elem   â”‚   å†…å­˜: 1B/elem    â”‚   å†…å­˜: 0.25B/elemâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                    â”‚
         â”‚                   â”‚                    â”‚
         â–¼                   â–¼                    â–¼
      æœ€é«˜ç²¾åº¦            4x å‹ç¼©             16x å‹ç¼©

ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€é™çº§:
Token è¿›å…¥ â†’ Hot Zone â†’ (64æ­¥å) â†’ Warm Zone â†’ (256æ­¥å) â†’ Cold Zone
```

**INT2Quantizer ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| scale | f32 | é‡åŒ–ç¼©æ”¾å› å­ |
| zero_point | i8 | é›¶ç‚¹ï¼ˆå¯¹ç§°é‡åŒ–ä¸º 0ï¼‰ |

**INT2 ç¼–ç è¡¨**:

| 2-bit å€¼ | æ˜ å°„æµ®ç‚¹ï¼ˆå¯¹ç§°ï¼‰ | è¯´æ˜ |
|----------|-----------------|------|
| 00 (0) | -1.5 * scale | æœ€å°å€¼ |
| 01 (1) | -0.5 * scale | è´Ÿå°å€¼ |
| 10 (2) | +0.5 * scale | æ­£å°å€¼ |
| 11 (3) | +1.5 * scale | æœ€å¤§å€¼ |

**INT2PackedBuffer ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Vec\<u8\> | æ‰“åŒ…æ•°æ®ï¼ˆ4 ä¸ª INT2 = 1 byteï¼‰ |
| scales | Vec\<f32\> | æ¯ç»„çš„ç¼©æ”¾å› å­ |
| group_size | usize | é‡åŒ–ç»„å¤§å°ï¼ˆé»˜è®¤ 128ï¼‰ |
| num_elements | usize | åŸå§‹å…ƒç´ æ•°é‡ |

**æ‰“åŒ…/è§£åŒ…æ“ä½œ**:

| æ“ä½œ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| pack_4_int2 | byte = (a<<6) \| (b<<4) \| (c<<2) \| d | 4 ä¸ª INT2 æ‰“åŒ…ä¸º 1 byte |
| unpack_4_int2 | a=(byte>>6)&3, b=(byte>>4)&3, c=(byte>>2)&3, d=byte&3 | ä» 1 byte è§£åŒ… 4 ä¸ª INT2 |

**ProgressiveKVCache ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| hot_k | Buffer\<f16\> | çƒ­åŒº Keyï¼ˆFP16ï¼‰ |
| hot_v | Buffer\<f16\> | çƒ­åŒº Valueï¼ˆFP16ï¼‰ |
| warm_k | Buffer\<i8\> | æ¸©åŒº Keyï¼ˆINT8ï¼‰ |
| warm_v | Buffer\<i8\> | æ¸©åŒº Valueï¼ˆINT8ï¼‰ |
| cold_k | INT2PackedBuffer | å†·åŒº Keyï¼ˆINT2ï¼‰ |
| cold_v | INT2PackedBuffer | å†·åŒº Valueï¼ˆINT2ï¼‰ |
| config | ProgressiveQuantConfig | é…ç½® |

**ProgressiveQuantConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| hot_size | usize | 64 | çƒ­åŒºå¤§å° |
| warm_size | usize | 192 | æ¸©åŒºå¤§å°ï¼ˆ64-256ï¼‰ |
| group_size | usize | 128 | INT2 é‡åŒ–ç»„å¤§å° |
| enable_int2 | bool | true | æ˜¯å¦å¯ç”¨ INT2 å†·åŒº |
| enable_evicpress | bool | true | å¯ç”¨ EvicPress è”åˆç­–ç•¥ |

**EvicPressConfig ç»“æ„ï¼ˆè”åˆå‹ç¼©é©±é€ç­–ç•¥ï¼‰**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| max_cache_size | usize | 4096 | KV Cache æœ€å¤§ token æ•° |
| eviction_threshold | f32 | 0.9 | ç¼“å­˜å ç”¨ç‡è¾¾åˆ°æ­¤å€¼æ—¶è§¦å‘é©±é€ |
| importance_decay | f32 | 0.99 | ä½ç½®è¡°å‡å› å­ï¼ˆè¶Šè¿œè¶Šå°ï¼‰ |
| attention_weight | f32 | 0.6 | æ³¨æ„åŠ›åˆ†æ•°æƒé‡ |
| semantic_weight | f32 | 0.4 | è¯­ä¹‰é‡è¦æ€§æƒé‡ |
| min_keep_tokens | usize | 128 | æœ€å°‘ä¿ç•™ token æ•°ï¼ˆsink + å…³é”® tokenï¼‰ |

**EvicPress å†³ç­–æµç¨‹**:

| ç¼“å­˜çŠ¶æ€ | å†³ç­– | æ“ä½œ |
|----------|------|------|
| å ç”¨ç‡ < 50% | æ— æ“ä½œ | æ­£å¸¸å†™å…¥ |
| 50% â‰¤ å ç”¨ç‡ < 90% | å‹ç¼© | ä½é‡è¦æ€§ token é™çº§ï¼ˆFP16â†’INT8â†’INT2ï¼‰ |
| å ç”¨ç‡ â‰¥ 90% | è”åˆ | åŒæ—¶å‹ç¼© + é©±é€æœ€ä½é‡è¦æ€§ token |

**å†…å­˜èŠ‚çœåˆ†æ**:

| é…ç½® | 1K tokens å†…å­˜ | vs FP16 |
|------|---------------|---------|
| å…¨ FP16 | 4 KB | åŸºå‡† |
| INT8 | 1 KB | 4x |
| INT2 | 0.25 KB | 16x |
| æ¸è¿›å¼ï¼ˆ64+192+å†·åŒºï¼‰ | 0.5-1 KB | 4-8x |

---

### ARCH-OP-012: Infinite Retrieval é•¿ä¸Šä¸‹æ–‡

> **âš ï¸ å®ç°çŠ¶æ€**ï¼šâŒ æœªå®ç°ï¼ˆè®¾è®¡æ–‡æ¡£ï¼‰
> - ä¾èµ– ARCH-OP-007 StreamingLLMï¼ˆä¹Ÿæœªå®ç°ï¼‰

**è®¾è®¡ç›®æ ‡**: è¶…é•¿ä¸Šä¸‹æ–‡æ£€ç´¢å¢å¼ºï¼Œæ”¯æŒ 100K+ token ä¸Šä¸‹æ–‡ï¼Œä¸ä¸¢å¤±è¿œç«¯é‡è¦ä¿¡æ¯

**æ ¸å¿ƒåŸç†**:
- ä¸ StreamingLLMï¼ˆARCH-OP-007ï¼‰é…åˆä½¿ç”¨
- å¯¹è¢«æ·˜æ±°çš„ KV å»ºç«‹æ£€ç´¢ç´¢å¼•
- ç”Ÿæˆæ—¶æŒ‰éœ€æ£€ç´¢å¹¶åŠ è½½å†å²é‡è¦ token

**æ¶æ„è®¾è®¡**:

```
Infinite Context Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Active Context (StreamingLLM)           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  Sinks   â”‚        Sliding Window                â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  (N=4)   â”‚        (L=512)                       â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â”‚ è¶…å‡ºçª—å£çš„ KV                    â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Historical KV Store                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  KV Index  â”‚        KV Data                   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  (HNSW/    â”‚        (Compressed or Paged)     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚   FAISS)   â”‚                                  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â”‚ Query-based Retrieval           â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Retrieved Context Injection             â”‚   â”‚
â”‚  â”‚  Attention(Q, [K_sink | K_window | K_retrieved])     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**HistoricalKVStore ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| kv_data | PagedBuffer | åˆ†é¡µå­˜å‚¨çš„å†å² KV |
| index | KVIndex | æ£€ç´¢ç´¢å¼•ï¼ˆåŸºäº Key çš„ embeddingï¼‰ |
| metadata | Vec\<KVMetadata\> | KV å…ƒæ•°æ®ï¼ˆposition, importanceï¼‰ |
| config | RetrievalConfig | æ£€ç´¢é…ç½® |

**KVIndex ç»“æ„ï¼ˆå¯é€‰å®ç°ï¼‰**:

| å®ç°æ–¹å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| LinearScan | çº¿æ€§æ‰«æ | å†å² KV < 1000 |
| HNSW | å±‚æ¬¡åŒ–å¯¼èˆªå°ä¸–ç•Œå›¾ | å†å² KV > 1000 |
| LSH | å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ | è¶…å¤§è§„æ¨¡ï¼Œè¿‘ä¼¼æ£€ç´¢ |

**KVMetadata ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| position | usize | åŸå§‹ä½ç½® |
| importance | f32 | é‡è¦æ€§åˆ†æ•°ï¼ˆåŸºäº attention æƒé‡ï¼‰ |
| layer_mask | u32 | å­˜å‚¨äº†å“ªäº›å±‚çš„ KVï¼ˆä½æ©ç ï¼‰ |

**RetrievalConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| top_k | usize | 32 | æ¯æ¬¡æ£€ç´¢è¿”å› token æ•° |
| retrieval_interval | usize | 64 | æ£€ç´¢è§¦å‘é—´éš”ï¼ˆæ¯ N æ­¥æ£€ç´¢ä¸€æ¬¡ï¼‰ |
| importance_threshold | f32 | 0.1 | é‡è¦æ€§é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼ä¸å­˜å‚¨ï¼‰ |
| max_historical_size | usize | 100000 | æœ€å¤§å†å²å­˜å‚¨ |

**InfiniteContext ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| streaming_cache | StreamingKVCache | æ´»è·ƒä¸Šä¸‹æ–‡ï¼ˆARCH-OP-007ï¼‰ |
| historical_store | HistoricalKVStore | å†å²å­˜å‚¨ |
| retriever | ContextRetriever | æ£€ç´¢å™¨ |

**InfiniteContext æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `append` | k: Buffer, v: Buffer, importance: f32 | () | è¿½åŠ  KVï¼ˆè‡ªåŠ¨ç®¡ç†å­˜å‚¨ä½ç½®ï¼‰ |
| `get_attention_kv` | query: Buffer | (Buffer, Buffer) | è·å– Attention KVï¼ˆsink + window + retrievedï¼‰ |
| `evict_to_historical` | k: Buffer, v: Buffer, meta: KVMetadata | () | å°†æ·˜æ±°çš„ KV å­˜å…¥å†å² |
| `retrieve` | query: Buffer, top_k: usize | Vec\<(Buffer, Buffer)\> | æ£€ç´¢ç›¸å…³å†å² KV |

**ä¸å…¶ä»–ç»„ä»¶é›†æˆ**:

| é›†æˆç‚¹ | æ–¹å¼ |
|--------|------|
| StreamingLLM | å…±äº« sink + windowï¼Œæ·˜æ±°æ—¶è§¦å‘ evict_to_historical |
| PM-KVQ | å†å² KV å¯ä½¿ç”¨ INT2 é‡åŒ–å­˜å‚¨ |
| PagedAttention | historical_store ä½¿ç”¨ Paged å­˜å‚¨ |

**æ€§èƒ½å‚æ•°**:

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ | 100K+ tokens |
| æ£€ç´¢å»¶è¿Ÿ | < 1msï¼ˆGPUï¼‰, < 10msï¼ˆCPUï¼‰ |
| å†å²å­˜å‚¨å¼€é”€ | < 10% åŸå§‹ KV å†…å­˜ |

---

### ARCH-OP-013: Assisted Generation / Self-Speculative Decoding

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°ï¼ˆMedusaï¼‰
> - ä»£ç ä½ç½®ï¼š`src/ops/medusa/`
> - æ ¸å¿ƒæ¨¡å—ï¼š`engine.rs`(262è¡Œ), `head.rs`(223è¡Œ), `cache.rs`(NgramCache)
> - å¯¼å‡ºï¼š`MedusaEngine`, `MedusaHead`, `MedusaDraft`, `NgramCache`, `AssistedGenerationConfig`

**è®¾è®¡ç›®æ ‡**: ä½¿ç”¨è¾…åŠ©æ¨¡å‹æˆ–æ¨¡å‹è‡ªèº«ä½å±‚åŠ é€Ÿç”Ÿæˆï¼Œæ— éœ€ç‹¬ç«‹è‰ç¨¿æ¨¡å‹ï¼Œç›®æ ‡ 1.5-2x å»¶è¿Ÿé™ä½

**åŸºäºè®ºæ–‡**:
- [Draft & Verify](https://arxiv.org/abs/2309.08168) - Lossless Large Language Model Acceleration
- [Medusa](https://arxiv.org/abs/2401.10774) - Multiple Decode Heads
- [Lookahead Decoding](https://arxiv.org/abs/2402.02057) - N-gram é¢„æµ‹åŠ é€Ÿ

**æ ¸å¿ƒåŸç†**:
- **è¾…åŠ©å¤´æ–¹æ¡ˆï¼ˆMedusaï¼‰**ï¼šåœ¨ LLM é¡¶å±‚æ·»åŠ å¤šä¸ªè§£ç å¤´ï¼Œå¹¶è¡Œé¢„æµ‹æœªæ¥ token
- **N-gram é¢„æµ‹**ï¼šåˆ©ç”¨å†å² N-gram ç»Ÿè®¡é¢„æµ‹æœªæ¥ token
- **è‡ªæ¨æµ‹æ–¹æ¡ˆ**ï¼šä½¿ç”¨æ¨¡å‹æ—©æœŸå±‚çš„è¾“å‡ºä½œä¸ºè‰ç¨¿ï¼ˆä¸ LayerSkip é…åˆï¼‰
- **æ— æŸåŠ é€Ÿ**ï¼šéªŒè¯é˜¶æ®µä¿è¯è¾“å‡ºä¸åŸå§‹æ¨¡å‹å®Œå…¨ä¸€è‡´

**æ¶æ„è®¾è®¡**:

```
Assisted Generation Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Main LLM (Target Model)                    â”‚   â”‚
â”‚  â”‚  [Layer 0] â†’ [Layer 1] â†’ ... â†’ [Layer N-1] â†’ [LM Head]â”‚   â”‚
â”‚  â”‚                                        â”‚              â”‚   â”‚
â”‚  â”‚                                        â–¼              â”‚   â”‚
â”‚  â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚                              â”‚  Medusa Heads  â”‚      â”‚   â”‚
â”‚  â”‚                              â”‚  [H1][H2][H3]  â”‚      â”‚   â”‚
â”‚  â”‚                              â”‚   â†“   â†“   â†“   â”‚      â”‚   â”‚
â”‚  â”‚                              â”‚  t+1 t+2 t+3  â”‚      â”‚   â”‚
â”‚  â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Verification Phase                      â”‚   â”‚
â”‚  â”‚  Run full forward on [t+1, t+2, t+3] candidates     â”‚   â”‚
â”‚  â”‚  Accept matching tokens, reject divergent ones      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**MedusaHead ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| weights | Buffer | é¢„æµ‹æƒé‡: [hidden_dim, vocab_size] |
| position_offset | usize | é¢„æµ‹ä½ç½®åç§»ï¼ˆ1=ä¸‹ä¸€ä¸ªï¼Œ2=ä¸‹ä¸‹ä¸ªï¼‰ |
| temperature | f32 | é‡‡æ ·æ¸©åº¦ |

**AssistedGenerationConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| num_medusa_heads | usize | 3 | Medusa å¤´æ•°é‡ |
| speculation_depth | usize | 4 | æ¨æµ‹æ·±åº¦ |
| candidate_count | usize | 8 | å€™é€‰ token æ•°é‡ |
| use_ngram_draft | bool | true | æ˜¯å¦ä½¿ç”¨ N-gram è¾…åŠ©è‰ç¨¿ |
| ngram_size | usize | 3 | N-gram å¤§å° |
| tree_attention | bool | true | æ˜¯å¦ä½¿ç”¨æ ‘æ³¨æ„åŠ›éªŒè¯ï¼ˆé…åˆ DeFTï¼‰ |

**AssistedGeneration æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `generate_draft` | hidden: Buffer | Vec\<TokenId\> | ç”Ÿæˆè‰ç¨¿ token |
| `verify` | draft_tokens: Vec\<TokenId\> | (Vec\<TokenId\>, usize) | éªŒè¯å¹¶è¿”å›æ¥å—çš„ token |
| `update_ngram` | accepted_tokens: Vec\<TokenId\> | () | æ›´æ–° N-gram ç»Ÿè®¡ |

---

### ARCH-OP-014: Prompt Caching / KV Cache Reuse

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - ä»£ç ä½ç½®ï¼š`src/ops/prompt_cache.rs`ï¼ˆ1333 è¡Œï¼‰
> - å¯¼å‡ºï¼š`PromptCacheManager`, `PromptCacheEntry`, `BlendedKVCache`, `CacheHit`, `EvictionPolicy`, `StorageTier`

**è®¾è®¡ç›®æ ‡**: è·¨è¯·æ±‚å¤ç”¨ KV Cacheï¼Œå‡å°‘é‡å¤è®¡ç®—ï¼Œç›®æ ‡ 2-15x ååæå‡

**åŸºäºè®ºæ–‡**:
- [CacheBlend](https://arxiv.org/abs/2405.16444) (EuroSys'25 Best Paper) - 3.9x RAG ååæå‡
- [LMCache](https://arxiv.org/abs/2505.12125) - 15x ååï¼Œ2x å»¶è¿Ÿé™ä½
- [vLLM](https://arxiv.org/abs/2309.06180) - PagedAttention ä¸å‰ç¼€ç¼“å­˜

**æ ¸å¿ƒåŸç†**:
- **å‰ç¼€ç¼“å­˜**ï¼šç›¸åŒ prompt å‰ç¼€çš„ KV Cache å¯è·¨è¯·æ±‚å¤ç”¨
- **è¯­ä¹‰èåˆï¼ˆCacheBlendï¼‰**ï¼šä¸åŒçŸ¥è¯†ç‰‡æ®µçš„ KV é€šè¿‡ä½ç½®é‡ç¼–ç èåˆ
- **åˆ†å±‚å­˜å‚¨**ï¼šçƒ­ KV åœ¨ GPUï¼Œæ¸© KV åœ¨ CPUï¼Œå†· KV åœ¨ç£ç›˜/ç½‘ç»œ
- **å¼•ç”¨è®¡æ•°**ï¼šè‡ªåŠ¨ç®¡ç† KV ç”Ÿå‘½å‘¨æœŸï¼Œæ”¯æŒ CoWï¼ˆCopy-on-Writeï¼‰

**æ¶æ„è®¾è®¡**:

```
Prompt Caching Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Prompt Cache Manager                    â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚   Request A: "System: You are an assistant..."       â”‚   â”‚
â”‚  â”‚   Request B: "System: You are an assistant..."       â”‚   â”‚
â”‚  â”‚             â†˜      â†™                                 â”‚   â”‚
â”‚  â”‚        Cache Hit! Reuse KV                           â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Hierarchical KV Storage                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  GPU L1  â”‚  CPU L2  â”‚    Disk/Network L3       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  (Hot)   â”‚  (Warm)  â”‚    (Cold)                â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  1GB     â”‚  16GB    â”‚    Unlimited             â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              CacheBlend Position Reencoding          â”‚   â”‚
â”‚  â”‚  Knowledge A (pos 0-100) + Knowledge B (pos 0-50)   â”‚   â”‚
â”‚  â”‚  â†’ Merged with reencoded positions (0-150)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PromptCacheEntry ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| hash | u64 | Prompt å†…å®¹çš„å“ˆå¸Œå€¼ |
| kv_blocks | Vec\<KVBlockId\> | KV æ•°æ®å— ID åˆ—è¡¨ |
| token_count | usize | ç¼“å­˜çš„ token æ•°é‡ |
| ref_count | AtomicUsize | å¼•ç”¨è®¡æ•° |
| last_access | Instant | æœ€åè®¿é—®æ—¶é—´ |
| storage_tier | StorageTier | å­˜å‚¨å±‚çº§ï¼ˆGPU/CPU/Diskï¼‰ |

**PromptCacheConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| gpu_cache_size | usize | 1 GB | GPU ç¼“å­˜å¤§å° |
| cpu_cache_size | usize | 16 GB | CPU ç¼“å­˜å¤§å° |
| enable_disk_cache | bool | true | æ˜¯å¦å¯ç”¨ç£ç›˜ç¼“å­˜ |
| eviction_policy | EvictionPolicy | LRU | é©±é€ç­–ç•¥ |
| enable_cacheblend | bool | true | å¯ç”¨ CacheBlend è¯­ä¹‰èåˆ |
| min_prefix_length | usize | 64 | æœ€å°ç¼“å­˜å‰ç¼€é•¿åº¦ |
| hash_algorithm | HashAlgorithm | xxHash64 | å“ˆå¸Œç®—æ³• |

**PromptCacheManager æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `lookup` | prompt_tokens: Vec\<TokenId\> | Option\<CacheHit\> | æŸ¥æ‰¾å‰ç¼€ç¼“å­˜ |
| `insert` | prompt_tokens: Vec\<TokenId\>, kv: KVCache | CacheEntryId | æ’å…¥ç¼“å­˜ |
| `evict_lru` | target_size: usize | usize | é©±é€æœ€å°‘ä½¿ç”¨çš„æ¡ç›® |
| `blend_knowledge` | entries: Vec\<CacheEntryId\> | KVCache | CacheBlend èåˆå¤šä¸ªçŸ¥è¯†ç‰‡æ®µ |
| `prefetch` | entry_id: CacheEntryId | () | é¢„å–åˆ°é«˜å±‚å­˜å‚¨ |

---

### ARCH-OP-015: Chunked Prefill Attention

> **âœ… å®ç°çŠ¶æ€**ï¼šå·²å®ç°
> - ä»£ç ä½ç½®ï¼š`src/ops/chunked_prefill.rs`ï¼ˆ1048 è¡Œï¼‰
> - å¯¼å‡ºï¼š`ChunkedPrefillScheduler`, `ChunkConfig`, `PODAttentionConfig`, `PrefillRequest`, `DecodeRequest`, `ScheduledBatch`

**è®¾è®¡ç›®æ ‡**: åˆ†å—é¢„å¡«å……ä¸è§£ç é‡å æ‰§è¡Œï¼Œä¼˜åŒ–é•¿ prompt åœºæ™¯ï¼Œç›®æ ‡ 10-22% ååæå‡

**åŸºäºè®ºæ–‡**:
- [POD-Attention](https://arxiv.org/abs/2411.13369) (ASPLOS'25) - 22% ååæå‡
- [FlashInfer](https://flashinfer.ai/) - Customizable Attention Engine
- [Sarathi](https://arxiv.org/abs/2308.16369) - Chunked Prefill ä¸æµæ°´çº¿

**æ ¸å¿ƒåŸç†**:
- **åˆ†å—é¢„å¡«å……**ï¼šå°†é•¿ prompt åˆ†æˆå¤šä¸ª chunkï¼Œé€å—è®¡ç®— KV
- **Prefill-Decode é‡å **ï¼šprefill æŸäº›è¯·æ±‚çš„åŒæ—¶ decode å…¶ä»–è¯·æ±‚
- **åŠ¨æ€è°ƒåº¦**ï¼šæ ¹æ® GPU åˆ©ç”¨ç‡åŠ¨æ€åˆ†é… prefill/decode èµ„æº
- **å†…å­˜æ•ˆç‡**ï¼šchunk çº§åˆ«çš„ KV ç®¡ç†ï¼Œé¿å…å¤§å—å†…å­˜åˆ†é…

**æ¶æ„è®¾è®¡**:

```
Chunked Prefill Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Request Queue                           â”‚   â”‚
â”‚  â”‚  [Prefill A: 8K tokens] [Decode B] [Decode C] ...   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Chunk Scheduler                         â”‚   â”‚
â”‚  â”‚  Prefill A: [Chunk 0-2K] [Chunk 2K-4K] [Chunk 4K-6K]â”‚   â”‚
â”‚  â”‚             [Chunk 6K-8K]                            â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Interleave with decodes:                           â”‚   â”‚
â”‚  â”‚  [Chunk 0] â†’ [Decode B,C] â†’ [Chunk 1] â†’ [Decode] ...â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              POD-Attention Kernel                    â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
â”‚  â”‚  â”‚   Prefill SMs    â”‚    Decode SMs    â”‚            â”‚   â”‚
â”‚  â”‚  â”‚   (60%)          â”‚    (40%)         â”‚            â”‚   â”‚
â”‚  â”‚  â”‚   Chunk Attn     â”‚    Token Attn    â”‚            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Dynamic SM allocation based on load                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ChunkConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| chunk_size | usize | 2048 | æ¯ä¸ª chunk çš„ token æ•° |
| max_chunks_per_batch | usize | 4 | æ¯æ‰¹æœ€å¤§ chunk æ•° |
| interleave_decodes | bool | true | æ˜¯å¦ä¸ decode äº¤ç»‡æ‰§è¡Œ |
| dynamic_chunk_size | bool | true | æ ¹æ® prompt é•¿åº¦åŠ¨æ€è°ƒæ•´ chunk å¤§å° |

**PODAttentionConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| prefill_sm_ratio | f32 | 0.6 | Prefill åˆ†é…çš„ SM æ¯”ä¾‹ |
| decode_sm_ratio | f32 | 0.4 | Decode åˆ†é…çš„ SM æ¯”ä¾‹ |
| enable_dynamic_allocation | bool | true | åŠ¨æ€ SM åˆ†é… |
| min_sm_per_task | usize | 4 | æ¯ä»»åŠ¡æœ€å° SM æ•° |

**ChunkedPrefillScheduler ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| prefill_queue | VecDeque\<PrefillRequest\> | å¾…å¤„ç†çš„ prefill è¯·æ±‚ |
| decode_queue | VecDeque\<DecodeRequest\> | å¾…å¤„ç†çš„ decode è¯·æ±‚ |
| chunk_config | ChunkConfig | åˆ†å—é…ç½® |
| pod_config | PODAttentionConfig | POD-Attention é…ç½® |

**ChunkedPrefillScheduler æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `submit_prefill` | request: PrefillRequest | RequestId | æäº¤ prefill è¯·æ±‚ |
| `submit_decode` | request: DecodeRequest | RequestId | æäº¤ decode è¯·æ±‚ |
| `schedule_batch` | () | ScheduledBatch | è°ƒåº¦ä¸‹ä¸€æ‰¹æ¬¡ï¼ˆæ··åˆ prefill chunk + decodeï¼‰ |
| `execute_batch` | batch: ScheduledBatch | Vec\<Output\> | æ‰§è¡Œæ‰¹æ¬¡ |

**æ€§èƒ½å¯¹æ¯”**:

| åœºæ™¯ | æ—  Chunked Prefill | æœ‰ Chunked Prefill | æå‡ |
|------|-------------------|-------------------|------|
| é•¿ prompt (8K) | åŸºå‡† | +15% åå | POD-Attention é‡å  |
| æ··åˆè´Ÿè½½ | åŸºå‡† | +22% åå | èµ„æºåˆ©ç”¨ç‡æå‡ |
| æ‰¹å¤„ç†å»¶è¿Ÿ | åŸºå‡† | -30% P99 å»¶è¿Ÿ | æ›´å¹³æ»‘çš„è°ƒåº¦ |

---

## æ•°å€¼ç¨³å®šæ€§ç®—æ³•

### ARCH-ALGO-001: Kahan è¡¥å¿æ±‚å’Œ

**é—®é¢˜**: æµ®ç‚¹ç´¯åŠ è¯¯å·®éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ O(n)

**è§£å†³æ–¹æ¡ˆ**: Kahan ç®—æ³•å°†è¯¯å·®é™è‡³ O(1)

#### KahanAccumulator ç»“æ„

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| sum | Float | ç´¯åŠ ç»“æœ |
| c | Float | è¡¥å¿é¡¹ï¼ˆlost bitsï¼‰ |

#### add æ–¹æ³•æ­¥éª¤

| æ­¥éª¤ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| 1 | y = value - c | è¡¥å¿ä¸Šæ¬¡ä¸¢å¤±çš„ä½ |
| 2 | t = sum + y | å¯èƒ½ä¸¢å¤±ä½ä½ |
| 3 | c = (t - sum) - y | è®¡ç®—æœ¬æ¬¡ä¸¢å¤±çš„ä½ |
| 4 | sum = t | æ›´æ–°ç´¯åŠ ç»“æœ |

**ä½ç½®**: `ops/stable_accumulator.rs`

### ARCH-ALGO-002: Log-Space Softmax

**é—®é¢˜**: 2M+ token åºåˆ—çš„ exp() ä¼šæº¢å‡º

**è§£å†³æ–¹æ¡ˆ**: åœ¨å¯¹æ•°ç©ºé—´è®¡ç®—ï¼Œä½¿ç”¨ log-add-exp æŠ€å·§

#### LogSpaceSoftmax ç»“æ„

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| m | f32 | å½“å‰æœ€å¤§å€¼ |
| log_l | f32 | log(Î£ exp(x_i - m)) |

#### update æ–¹æ³•æµç¨‹

| æ¡ä»¶ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| x > m | æ›´æ–° log_l å’Œ m | å‘ç°æ–°æœ€å¤§å€¼ï¼Œé‡æ–°è®¡ç®— |
| x â‰¤ m | log_add_exp | ä½¿ç”¨ log-add-exp ç´¯åŠ  |

**ä½ç½®**: `ops/softmax.rs`

### ARCH-ALGO-003: åˆ†å±‚ç´¯åŠ å™¨

**é—®é¢˜**: è¶…é•¿åºåˆ—å³ä½¿ä½¿ç”¨ Kahan ä¹Ÿä¼šæœ‰è¯¯å·®ç´¯ç§¯

**è§£å†³æ–¹æ¡ˆ**: å¤šçº§åˆ†å±‚ç´¯åŠ ï¼Œå®šæœŸåˆå¹¶éƒ¨åˆ†å’Œ

#### HierarchicalAccumulator ç»“æ„

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| levels | Vec\<KahanAccumulator\> | å¤šçº§ç´¯åŠ å™¨ |
| counts | Vec\<usize\> | å„çº§è®¡æ•° |
| config | AccumulatorConfig | é…ç½®å‚æ•° |

**ä½ç½®**: `ops/stable_accumulator.rs`

---

## ä¸­é—´æ€ç¼–è¯‘æ¶æ„ï¼ˆFat Binaryï¼‰

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. æ‰€æœ‰åç«¯çš„ kernel é¢„ç¼–è¯‘ä¸ºä¸­é—´æ€ï¼ˆPTX/HSACO/metallibï¼‰
2. ä¸­é—´æ€é€šè¿‡ `include_bytes!` åµŒå…¥åˆ°å¯æ‰§è¡Œæ–‡ä»¶
3. è¿è¡Œæ—¶é€šè¿‡ Driver API åŠ è½½ä¸­é—´æ€å¹¶æ‰§è¡Œ
4. æ— ç¼–è¯‘æ—¶é“¾æ¥ä¾èµ–ï¼Œåªéœ€ç›®æ ‡å¹³å°çš„ GPU é©±åŠ¨

### ç¼–è¯‘æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kernel ç¼–è¯‘æµç¨‹ï¼ˆCI/ç¦»çº¿ï¼‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CUDA Kernel (.cu)     â”€â”€nvcc -ptxâ”€â”€â†’     PTX (.ptx)     â”€â”€â”        â”‚
â”‚  HIP Kernel (.hip)     â”€â”€hipcc --gencoâ”€â”€â†’ HSACO (.hsaco)   â”œâ†’ embed â”‚
â”‚  Metal Shader (.metal) â”€â”€xcrun metallibâ”€â†’ metallib (.metallib)      â”‚
â”‚  WGSL Shader (.wgsl)   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç›´æ¥åµŒå…¥ï¼ˆinclude_str!ï¼‰â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¿è¡Œæ—¶åŠ è½½ï¼ˆç”¨æˆ·æœºå™¨ï¼‰                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åµŒå…¥çš„ PTX      + CUDA Driver API (cudarc)  â†’ NVIDIA GPU æ‰§è¡Œ      â”‚
â”‚  åµŒå…¥çš„ HSACO    + HSA Runtime (libloading)  â†’ AMD GPU æ‰§è¡Œ         â”‚
â”‚  åµŒå…¥çš„ metallib + Metal Framework           â†’ Apple GPU æ‰§è¡Œ       â”‚
â”‚  åµŒå…¥çš„ WGSL     + wgpu è¿è¡Œæ—¶ç¼–è¯‘           â†’ è·¨å¹³å° GPU æ‰§è¡Œ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Driver API vs Runtime API

| å±‚çº§ | NVIDIA | AMD | Apple | ç‰¹ç‚¹ |
|------|--------|-----|-------|------|
| **Driver API** | `libcuda.so` | `libhsa-runtime64.so` | Metal.framework | åªéœ€é©±åŠ¨ |
| **Runtime API** | `libcudart.so` | `libamdhip64.so` | - | éœ€è¦å¼€å‘å·¥å…·åŒ… |

**gllm-kernels ä½¿ç”¨ Driver API**ï¼šç”¨æˆ·æœºå™¨åªéœ€å®‰è£… GPU é©±åŠ¨ï¼Œæ— éœ€å®Œæ•´å¼€å‘å·¥å…·åŒ…ã€‚

---

## ç›®å½•ç»“æ„

```
gllm-kernels/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                    # å…¬å…±å¯¼å‡º
â”‚   â”œâ”€â”€ backend.rs                # åç«¯ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ runtime_detection.rs      # è¿è¡Œæ—¶æ£€æµ‹
â”‚   â”œâ”€â”€ kernel_dispatcher.rs      # é›¶æˆæœ¬æ´¾å‘
â”‚   â”œâ”€â”€ types.rs                  # å…¬å…±ç±»å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ cuda_kernels/             # CUDA åç«¯ï¼ˆç»Ÿä¸€ç»“æ„ï¼‰
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ runtime.rs            # CUDA Driver API
â”‚   â”‚   â”œâ”€â”€ flash_attn.rs
â”‚   â”‚   â”œâ”€â”€ paged_attn.rs
â”‚   â”‚   â””â”€â”€ kernels/
â”‚   â”‚       â”œâ”€â”€ flash_attention.ptx
â”‚   â”‚       â””â”€â”€ paged_attention.ptx
â”‚   â”‚
â”‚   â”œâ”€â”€ hip_kernels/              # ROCm åç«¯ï¼ˆHSA Runtime Onlyï¼‰
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ hsa_runtime.rs        # HSA Runtime åŠ¨æ€åŠ è½½
â”‚   â”‚   â”œâ”€â”€ hsa_flash_attn.rs     # HSA Flash Attention
â”‚   â”‚   â”œâ”€â”€ hsa_paged_attn.rs     # HSA Paged Attention
â”‚   â”‚   â””â”€â”€ kernels/
â”‚   â”‚       â”œâ”€â”€ flash_attention.hsaco
â”‚   â”‚       â””â”€â”€ paged_attention.hsaco
â”‚   â”‚
â”‚   â”œâ”€â”€ metal_kernels/            # Metal åç«¯
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ metal_runtime.rs      # Metal Framework æŠ½è±¡
â”‚   â”‚   â”œâ”€â”€ metallib_loader.rs    # MetallibCollection ç»Ÿä¸€åŠ è½½
â”‚   â”‚   â”œâ”€â”€ flash_attn.rs         # Flash Attention
â”‚   â”‚   â”œâ”€â”€ paged_attn.rs         # Paged Attention
â”‚   â”‚   â””â”€â”€ kernels/
â”‚   â”‚       â”œâ”€â”€ flash_attention.metallib
â”‚   â”‚       â”œâ”€â”€ flash_attention.metal
â”‚   â”‚       â”œâ”€â”€ paged_attention.metallib
â”‚   â”‚       â””â”€â”€ paged_attention.metal
â”‚   â”‚
â”‚   â”œâ”€â”€ wgpu_kernels/             # WGPU åç«¯
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ runtime.rs            # wgpu æŠ½è±¡
â”‚   â”‚   â”œâ”€â”€ flash_attn.rs
â”‚   â”‚   â”œâ”€â”€ paged_attn.rs
â”‚   â”‚   â””â”€â”€ kernels/
â”‚   â”‚       â”œâ”€â”€ flash_attention.wgsl
â”‚   â”‚       â””â”€â”€ paged_attention.wgsl
â”‚   â”‚
â”‚   â”œâ”€â”€ ops/                      # çº¯ Rust ç®—å­å®ç°ï¼ˆä¸åç«¯æ— å…³ï¼‰
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ softmax.rs            # Log-Space Softmax
â”‚   â”‚   â”œâ”€â”€ stable_accumulator.rs # Kahan/å±‚çº§ç´¯åŠ å™¨
â”‚   â”‚   â”œâ”€â”€ linear.rs             # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ rms_norm.rs           # RMS å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ layer_norm.rs         # Layer å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ activations.rs        # æ¿€æ´»å‡½æ•° (SiLU, GELU, ReLU, etc.)
â”‚   â”‚   â”œâ”€â”€ rope.rs               # RoPE ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ sampling.rs           # é‡‡æ ·ç®—å­
â”‚   â”‚   â”œâ”€â”€ moe_routing.rs        # MoE è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ embedding.rs          # åµŒå…¥ç®—å­
â”‚   â”‚   â”œâ”€â”€ engram*.rs            # Engram æ¡ä»¶è®°å¿†
â”‚   â”‚   â”œâ”€â”€ eagle3/               # EAGLE-3 è‡ªé€‚åº”è‰ç¨¿ (ARCH-OP-008)
â”‚   â”‚   â”œâ”€â”€ spec_ee/              # SpecEE/LayerSkip (ARCH-OP-009)
â”‚   â”‚   â”œâ”€â”€ medusa/               # Medusa è¾…åŠ©ç”Ÿæˆ (ARCH-OP-013)
â”‚   â”‚   â”œâ”€â”€ flash_tree_attn.rs    # Flash Tree-attention (ARCH-OP-010)
â”‚   â”‚   â”œâ”€â”€ int2_quantizer.rs     # INT2 é‡åŒ– (ARCH-OP-011)
â”‚   â”‚   â”œâ”€â”€ evic_press.rs         # EvicPress é©±é€ (ARCH-OP-011)
â”‚   â”‚   â”œâ”€â”€ prompt_cache.rs       # Prompt Cache (ARCH-OP-014)
â”‚   â”‚   â””â”€â”€ chunked_prefill.rs    # Chunked Prefill (ARCH-OP-015)
â”‚   â”‚
â”‚   â””â”€â”€ comm/                     # é€šä¿¡åç«¯
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ nccl.rs
â”‚       â””â”€â”€ tcp.rs
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compile_cuda_kernels.sh   # CUDA PTX ç¼–è¯‘
â”‚   â”œâ”€â”€ compile_hip_kernels.sh    # HIP HSACO ç¼–è¯‘
â”‚   â””â”€â”€ compile_metal_kernels.sh  # Metal metallib ç¼–è¯‘
â”‚
â”œâ”€â”€ SPEC/                         # è®¾è®¡æ–‡æ¡£
â””â”€â”€ benches/                      # æ€§èƒ½æµ‹è¯•
```

---

## ä¸ gllm çš„é›†æˆ

### ARCH-INT-001: é›†æˆæ¶æ„

> âš ï¸ **Burn-Free æ¶æ„**ï¼šæ ¹æ® ADR-001ï¼Œæœ¬é¡¹ç›®å·²å®Œå…¨ç§»é™¤ Burn ä¾èµ–ï¼Œ
> ä½¿ç”¨åŸå§‹åˆ‡ç‰‡ `&[T]` + KernelDispatcher å®ç°é›¶æˆæœ¬æŠ½è±¡ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           gllm                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Layer                                                         â”‚
â”‚    â””â”€â”€ åŸå§‹åˆ‡ç‰‡ &[T] + WeightMatrix/WeightVector ç”¨äºæƒé‡åŠ è½½         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Attention Layer                                                     â”‚
â”‚    â”œâ”€â”€ æ£€æµ‹ gllm-kernels å¯ç”¨æ€§ï¼ˆè¿è¡Œæ—¶åç«¯æ£€æµ‹ï¼‰                     â”‚
â”‚    â”œâ”€â”€ å¯ç”¨ â†’ è°ƒç”¨ KernelDispatcher::flash_attention()               â”‚
â”‚    â””â”€â”€ ä¸å¯ç”¨ â†’ fallback åˆ° CPU å‚è€ƒå®ç°                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ è°ƒç”¨
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        gllm-kernels                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  KernelDispatcher                                                    â”‚
â”‚    â”œâ”€â”€ è¿è¡Œæ—¶é€‰æ‹©åç«¯ï¼ˆCUDA/ROCm/Metal/WGPU/CPUï¼‰                    â”‚
â”‚    â”œâ”€â”€ enum + match é›¶æˆæœ¬æ´¾å‘ï¼ˆæ—  vtableï¼‰                          â”‚
â”‚    â””â”€â”€ ä½¿ç”¨ 2M ä¸Šä¸‹æ–‡ä¼˜åŒ–ç®—æ³•                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Feature é…ç½®ï¼ˆæœ€å°åŒ–ï¼‰

> âš ï¸ **åŒæ­¥çŠ¶æ€**ï¼šä»¥ä¸‹é…ç½®ä¸ Cargo.toml ä¿æŒåŒæ­¥ï¼ˆ2025-01ï¼‰

```toml
[features]
# é»˜è®¤ï¼šå®Œæ•´æ”¯æŒ
default = ["full"]

# å®Œæ•´ç‰ˆï¼šæ‰€æœ‰åç«¯ + æ‰€æœ‰ kernelï¼ˆFat Binaryï¼Œå…¨éƒ¨åµŒå…¥ï¼‰
full = ["all-backends", "all-kernels"]

# æ‰€æœ‰åç«¯ï¼ˆè¿è¡Œæ—¶æ£€æµ‹ï¼ŒåŠ¨æ€åŠ è½½ï¼‰
all-backends = []

# æ‰€æœ‰è‡ªå®šä¹‰ kernel
all-kernels = []

# Fusion æ”¯æŒï¼ˆå·²ç§»é™¤ Burn ä¾èµ–ï¼Œä¿ç•™ feature ä»¥å…¼å®¹ï¼‰
fusion = []

# NCCL å¤š GPUï¼ˆéœ€è¦ CUDA ç¯å¢ƒï¼‰
nccl = ["cudarc/nccl"]

# RCCL å¤š GPUï¼ˆéœ€è¦ ROCm ç¯å¢ƒï¼‰
rccl = []

# Flash Attention v3 ä¼˜åŒ–ï¼ˆHopper+ï¼‰
flash-attention-v3 = [
    "flash-attention-v3-wgmma",
    "flash-attention-v3-async",
    "flash-attention-v3-fp8",
    "flash-attention-v3-block-quant",
]

# ç²¾ç®€ç‰ˆï¼ˆä»… CPUï¼Œç”¨äºæµ‹è¯•/CIï¼‰
minimal = []

# Fat Binary é¢„ç¼–è¯‘å†…æ ¸ï¼ˆåµŒå…¥ ~10MBï¼‰
embedded-kernels = []

# è¿è¡Œæ—¶ä¸‹è½½å†…æ ¸ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶ä» GitHub Release ä¸‹è½½ï¼‰
download-kernels = []
```

---

## ARCH-EMBED-001: Embedding/Rerank å¿«è·¯å¾„æ¶æ„

### æ ¸å¿ƒç›®æ ‡

åŸºäºå­¦æœ¯å‰æ²¿ç ”ç©¶å®ç° Embedding å’Œ Rerank çš„ç¡¬ä»¶åŠ é€Ÿå¿«è·¯å¾„ï¼š
- **Binary Quantization**: 1-bit embeddingï¼ŒPOPCNT+SIMDï¼Œ32x ååæå‡
- **Int8 Quantization**: AVX512-VNNI/CUDA INT8ï¼Œ4x ååæå‡
- **Int4 Packed**: 2 values/byteï¼Œ8x å†…å­˜å¸¦å®½æå‡
- **Matryoshka Truncation**: è¿è¡Œæ—¶ç»´åº¦é€‰æ‹©ï¼ˆ1024â†’512â†’256â†’128ï¼‰
- **ä¸‰é˜¶æ®µ Rerank**: Binary ç²—ç­› â†’ Int8 ç²¾æ’ â†’ Cross-encoder ç»ˆæ’

### å®é™…ç›®å½•ç»“æ„

> âš ï¸ **å®ç°ä½ç½®**ï¼šEmbedding æ“ä½œç»Ÿä¸€åœ¨ `ops/embedding.rs`ï¼Œä½¿ç”¨çº¯ Rust + SIMD å®ç°ã€‚

```
src/ops/
â”œâ”€â”€ embedding.rs                # ç»Ÿä¸€çš„ Embedding æ“ä½œæ¨¡å—
â”‚   â”œâ”€â”€ BinaryIpConfig          # Binary Quantization é…ç½®
â”‚   â”œâ”€â”€ Int8DotConfig           # Int8 ç‚¹ç§¯é…ç½®
â”‚   â”œâ”€â”€ Int4PackedConfig        # Int4 æ‰“åŒ…é…ç½®
â”‚   â”œâ”€â”€ MatryoshkaConfig        # Matryoshka æˆªæ–­é…ç½®
â”‚   â”œâ”€â”€ RerankPipelineConfig    # ä¸‰é˜¶æ®µ Rerank é…ç½®
â”‚   â””â”€â”€ çº¯ Rust SIMD å®ç°       # æ— éœ€å•ç‹¬ GPU kernel
```

**è®¾è®¡å†³ç­–**ï¼š
- Embedding æ“ä½œä¸»è¦æ˜¯å†…å­˜å¸¦å®½å—é™ï¼Œçº¯ Rust + SIMD å·²è¶³å¤Ÿé«˜æ•ˆ
- ä½¿ç”¨ `packed_simd` / æ‰‹åŠ¨ SIMD å†…è”æ±‡ç¼–ä¼˜åŒ–
- é¿å…ç‹¬ç«‹ kernel æ¨¡å—çš„é¢å¤–å¤æ‚åº¦

### ARCH-EMBED-002: Binary Quantization Inner Product

**å­¦æœ¯æ¥æº**: Matryoshka Quantization (2025.02), SimSIMD

**æ ¸å¿ƒåŸç†**:
- å°† f32 embedding é‡åŒ–ä¸º 1-bitï¼ˆsign bitï¼‰
- ä½¿ç”¨ POPCNT æŒ‡ä»¤è®¡ç®— Hamming distance
- è½¬æ¢ä¸º cosine ç›¸ä¼¼åº¦è¿‘ä¼¼

**æ•°å­¦åŸºç¡€**:
```
# åŸå§‹å†…ç§¯
IP(a, b) = Î£ aáµ¢ * báµ¢

# Binary é‡åŒ–
q(x) = sign(x) âˆˆ {-1, +1} â†’ å­˜å‚¨ä¸º 0/1

# Binary å†…ç§¯ï¼ˆPOPCNTï¼‰
IP_bin(a, b) = popcount(a XOR b)
cosine â‰ˆ 1 - 2 * IP_bin / dim
```

**BinaryEmbedding ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Vec\<u64\> | æ‰“åŒ…çš„ bit å‘é‡ï¼ˆdim/64 ä¸ª u64ï¼‰ |
| dim | usize | åŸå§‹ç»´åº¦ |

**BinaryEmbedding æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `from_f32` | embedding: slice\<f32\> | Self | ä» f32 embedding é‡åŒ– |
| `binary_inner_product` | other: Self | u32 | Binary å†…ç§¯ï¼ˆè¿”å› Hamming distanceï¼‰ |
| `batch_inner_product` | candidates: slice, dispatcher | Vec\<u32\> | æ‰¹é‡å†…ç§¯ï¼ˆGPU åŠ é€Ÿï¼‰ |

**å„åç«¯å®ç°**:

| åç«¯ | æŒ‡ä»¤/API | é¢„æœŸåå |
|------|----------|----------|
| CPU (x86) | `_mm_popcnt_u64` + AVX2 | 10B ops/s |
| CUDA | `__popc()` + warp reduce | 100B+ ops/s |
| ROCm | `__builtin_popcount` | 100B+ ops/s |
| Metal | `simd_popcnt` | 50B+ ops/s |
| WGPU | bit æ“ä½œæ¨¡æ‹Ÿ | 5B ops/s |

### ARCH-EMBED-003: Int8 Embedding Dot Product

**å­¦æœ¯æ¥æº**: PilotANN (2025.03), Intel VNNI

**æ ¸å¿ƒåŸç†**:
- f32 embedding é‡åŒ–ä¸º int8ï¼ˆscale+zero_pointï¼‰
- ä½¿ç”¨ VNNI/DOT4 æŒ‡ä»¤ 4x åŠ é€Ÿ
- åé‡åŒ–æ¢å¤ f32 ç»“æœ

**Int8Embedding ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Vec\<i8\> | é‡åŒ–åçš„ int8 å‘é‡ |
| scale | f32 | ç¼©æ”¾å› å­ |
| zero_point | i8 | é›¶ç‚¹åç§»ï¼ˆå¯¹ç§°é‡åŒ–æ—¶ä¸º 0ï¼‰ |
| dim | usize | åŸå§‹ç»´åº¦ |

**é‡åŒ–æ–¹æ³• - å¯¹ç§°é‡åŒ–**:

| æ­¥éª¤ | æ“ä½œ | å…¬å¼ |
|------|------|------|
| 1 | è®¡ç®—æœ€å¤§ç»å¯¹å€¼ | `max_abs = max(abs(embedding))` |
| 2 | è®¡ç®—ç¼©æ”¾å› å­ | `scale = max_abs / 127.0` |
| 3 | é‡åŒ–æ¯ä¸ªå…ƒç´  | `data[i] = round(embedding[i] / scale)` |
| 4 | è®¾ç½®é›¶ç‚¹ | `zero_point = 0`ï¼ˆå¯¹ç§°é‡åŒ–ï¼‰ |

**å„åç«¯å®ç°**:

| åç«¯ | æŒ‡ä»¤/API | ååæå‡ |
|------|----------|----------|
| CPU (AVX512) | `_mm512_dpbusd_epi32` (VNNI) | 4x |
| CPU (AVX2) | `_mm256_maddubs_epi16` | 2x |
| CUDA | `__dp4a()` (INT8 DP4A) | 4x |
| ROCm | `__builtin_amdgcn_sdot4` | 4x |
| Metal | `simd_dot` (int8x4) | 4x |
| WGPU | æ‰‹åŠ¨å±•å¼€ | 1.5x |

### ARCH-EMBED-004: Int4 Packed Embedding

**å­¦æœ¯æ¥æº**: Matryoshka Quantization multi-precision (int8â†’int4â†’int2)

**æ ¸å¿ƒåŸç†**:
- 2 ä¸ª int4 å€¼æ‰“åŒ…åˆ° 1 ä¸ª byte
- 8x å†…å­˜å¸¦å®½æå‡
- è®¡ç®—æ—¶è§£åŒ…ä¸º int8 æˆ– f16

**Int4PackedEmbedding ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Vec\<u8\> | æ‰“åŒ…æ•°æ®ï¼ˆdim/2 bytesï¼‰ |
| scale | f32 | ç¼©æ”¾å› å­ |
| dim | usize | åŸå§‹ç»´åº¦ |

**æ‰“åŒ…/è§£åŒ…æ“ä½œ**:

| æ“ä½œ | è¾“å…¥ | è¾“å‡º | å…¬å¼ |
|------|------|------|------|
| pack_int4 | a: i8, b: i8 | u8 | `((a & 0x0F) << 4) \| (b & 0x0F)` |
| unpack_int4 | packed: u8 | (i8, i8) | `high = (packed >> 4) - 8`, `low = (packed & 0x0F) - 8` |

> **æ³¨**ï¼šint4 èŒƒå›´ [-8, 7]ï¼Œå­˜å‚¨æ—¶åŠ  8 å˜ä¸º [0, 15] æ— ç¬¦å·ã€‚

### ARCH-EMBED-005: Matryoshka Dimension Truncation

**å­¦æœ¯æ¥æº**: Matryoshka Representation Learning (MRL)

**æ ¸å¿ƒåŸç†**:
- Embedding æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨åµŒå¥—æŸå¤±
- å‰ N ç»´åŒ…å«æœ€é‡è¦ä¿¡æ¯
- è¿è¡Œæ—¶æŒ‰éœ€æˆªæ–­ç»´åº¦ï¼ˆ1024â†’512â†’256â†’128ï¼‰

**MatryoshkaConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| full_dim | usize | åŸå§‹ç»´åº¦ |
| truncation_points | Vec\<usize\> | å¯ç”¨æˆªæ–­ç‚¹ï¼ˆå¿…é¡»æ˜¯è®­ç»ƒæ—¶ä½¿ç”¨çš„ï¼‰ |

**å¸¸è§é…ç½®**:

| é…ç½®å | full_dim | truncation_points |
|--------|----------|-------------------|
| default_1024 | 1024 | [1024, 512, 256, 128] |

**truncate æ“ä½œ**:

| æ–¹æ³• | è¾“å…¥ | è¾“å‡º | è¯´æ˜ |
|------|------|------|------|
| truncate | embedding: slice, target_dim: usize | slice[..target_dim] | é›¶æˆæœ¬åˆ‡ç‰‡ï¼Œä»…æ”¹å˜èŒƒå›´ |

**ä¸é‡åŒ–ç»“åˆ**:
```
é˜¶æ®µ1 (ç²—ç­›): Binary @ 128 dim â†’ 32x åŠ é€Ÿï¼Œ4bit/vector
é˜¶æ®µ2 (ç²¾æ’): Int8 @ 512 dim â†’ 4x åŠ é€Ÿï¼Œ512B/vector
é˜¶æ®µ3 (ç»ˆæ’): FP32 @ 1024 dim â†’ åŸºå‡†ç²¾åº¦ï¼Œ4KB/vector
```

### ARCH-EMBED-006: ä¸‰é˜¶æ®µ Rerank ç®¡é“

**å­¦æœ¯æ¥æº**: PE-Rank (2024.06), Cohere Rerank v3

**æµç¨‹è®¾è®¡**:
```
Query Embedding
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: Binary ç²—ç­› (GPU Kernel)                          â”‚
â”‚  - è¾“å…¥: Query (binary) vs 100K candidates (binary)       â”‚
â”‚  - ç®—æ³•: POPCNT Hamming distance                          â”‚
â”‚  - è¾“å‡º: Top-1000 candidates                              â”‚
â”‚  - è€—æ—¶: ~1ms (GPU), ~10ms (CPU)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: Int8 ç²¾æ’ (GPU Kernel)                            â”‚
â”‚  - è¾“å…¥: Query (int8) vs Top-1000 candidates (int8)       â”‚
â”‚  - ç®—æ³•: INT8 dot product (VNNI/DP4A)                     â”‚
â”‚  - è¾“å‡º: Top-100 candidates                               â”‚
â”‚  - è€—æ—¶: ~0.5ms (GPU), ~5ms (CPU)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: Cross-encoder ç»ˆæ’ (å¯é€‰ï¼ŒLLM è°ƒç”¨)               â”‚
â”‚  - è¾“å…¥: Query text + Top-100 passages                    â”‚
â”‚  - ç®—æ³•: BERT/T5 cross-attention                          â”‚
â”‚  - è¾“å‡º: Top-10 ranked results                            â”‚
â”‚  - è€—æ—¶: ~50ms (GPU)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RerankPipeline ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| binary_kernel | BinaryIpKernel | Binary å†…ç§¯ Kernel |
| int8_kernel | Int8DpKernel | Int8 ç‚¹ç§¯ Kernel |
| config | RerankConfig | ç®¡é“é…ç½® |

**RerankConfig ç»“æ„**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| stage1_topk | usize | 1000 | é˜¶æ®µ1è¾“å‡ºæ•°é‡ |
| stage2_topk | usize | 100 | é˜¶æ®µ2è¾“å‡ºæ•°é‡ |
| enable_cross_encoder | bool | false | æ˜¯å¦å¯ç”¨é˜¶æ®µ3ï¼ˆéœ€è¦ LLMï¼‰ |

**RerankPipeline æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `rerank` | query: Embedding, candidates: EmbeddingIndex, dispatcher | Vec\<RerankResult\> | æ‰§è¡Œä¸‰é˜¶æ®µ Rerank |

### ARCH-EMBED-007: å®ç°ç­–ç•¥

> âš ï¸ **çº¯ Rust å®ç°**ï¼šEmbedding æ“ä½œä½¿ç”¨çº¯ Rust + SIMDï¼Œæ— éœ€ GPU kernelã€‚

**è®¾è®¡ç†ç”±**ï¼š
- Embedding æ“ä½œæ˜¯å†…å­˜å¸¦å®½å—é™ï¼ŒCPU SIMD å·²è¶³å¤Ÿé«˜æ•ˆ
- é¿å…é¢å¤–çš„ GPU kernel æ¨¡å—å¤æ‚åº¦
- å‡å°‘ç¼–è¯‘æ—¶é—´å’ŒäºŒè¿›åˆ¶å¤§å°

**SIMD åŠ é€Ÿ**ï¼š
- Binary IP: ä½¿ç”¨ POPCNT æŒ‡ä»¤ï¼ˆ`std::arch::x86_64::_popcnt64`ï¼‰
- Int8 Dot: ä½¿ç”¨ AVX2/AVX512 å‘é‡åŒ–
- Int4 Packed: ä½¿ç”¨ä½æ“ä½œä¼˜åŒ–

### ARCH-EMBED-008: å…¬å¼€ API

**ops/embedding.rs æ¨¡å—å¯¼å‡º**:

| å¯¼å‡ºé¡¹ | ç±»å‹ | è¯´æ˜ |
|--------|------|------|
| BinaryIpConfig | ç»“æ„ | Binary å†…ç§¯é…ç½® |
| pack_binary_f32 | å‡½æ•° | f32 â†’ binary æ‰“åŒ… |
| binary_ip_hamming | å‡½æ•° | Hamming distance è®¡ç®— |
| binary_ip_hamming_simd | å‡½æ•° | SIMD åŠ é€Ÿç‰ˆæœ¬ |
| binary_ip_asymmetric | å‡½æ•° | éå¯¹ç§° Binary IP |
| Int8DotConfig | ç»“æ„ | Int8 ç‚¹ç§¯é…ç½® |
| quantize_to_int8 | å‡½æ•° | f32 â†’ int8 é‡åŒ– |
| int8_dot_product | å‡½æ•° | Int8 ç‚¹ç§¯ |
| int8_dot_product_unrolled | å‡½æ•° | å±•å¼€ä¼˜åŒ–ç‰ˆæœ¬ |
| Int4PackedConfig | ç»“æ„ | Int4 æ‰“åŒ…é…ç½® |
| pack_int4 | å‡½æ•° | Int4 æ‰“åŒ… |
| unpack_int4 | å‡½æ•° | Int4 è§£åŒ… |
| quantize_to_int4_packed | å‡½æ•° | f32 â†’ int4 æ‰“åŒ… |
| int4_packed_dot_product | å‡½æ•° | Int4 æ‰“åŒ…ç‚¹ç§¯ |
| MatryoshkaConfig | ç»“æ„ | Matryoshka é…ç½® |
| matryoshka_truncate | å‡½æ•° | ç»´åº¦æˆªæ–­ |
| select_matryoshka_dim | å‡½æ•° | é€‰æ‹©æˆªæ–­ç»´åº¦ |
| RerankPipelineConfig | ç»“æ„ | Rerank ç®¡é“é…ç½® |
| RerankResult | ç»“æ„ | Rerank ç»“æœ |
| rerank_binary_stage | å‡½æ•° | Binary é˜¶æ®µ |
| rerank_int8_stage | å‡½æ•° | Int8 é˜¶æ®µ |

**KernelDispatcher Embedding æ‰©å±•æ¥å£**:

| æ–¹æ³• | å‚æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|------|----------|------|
| `binary_inner_product_batch` | query: BinaryEmbedding, candidates: slice | Result\<Vec\<u32\>, Error\> | Binary å†…ç§¯æ‰¹é‡è®¡ç®— |
| `int8_dot_product_batch` | query: Int8Embedding, candidates: slice | Result\<Vec\<i32\>, Error\> | Int8 ç‚¹ç§¯æ‰¹é‡è®¡ç®— |
| `int4_dot_product_batch` | query: Int4PackedEmbedding, candidates: slice | Result\<Vec\<i32\>, Error\> | Int4 ç‚¹ç§¯æ‰¹é‡è®¡ç®— |

---

## ARCH-API-001: ç»Ÿä¸€æ³›å‹ç®—å­ API è§„èŒƒï¼ˆğŸš¨ FROZEN - é›¶æˆæœ¬é“å¾‹ï¼‰

### æ ¸å¿ƒåŸåˆ™

**æ‰€æœ‰å®ç°å¿…é¡»æ˜¯é›¶æˆæœ¬çš„ï¼Œæ‰€æœ‰ç®—å­å¿…é¡»ä½¿ç”¨æ³›å‹ `<T: Float>`**ã€‚

### é›¶æˆæœ¬ä¿è¯æœºåˆ¶

| æœºåˆ¶ | è¯´æ˜ | å¼€é”€ |
|------|------|------|
| æ³›å‹å•æ€åŒ– | `<T: Float>` ç¼–è¯‘æ—¶å±•å¼€ä¸ºå…·ä½“ç±»å‹å‡½æ•° | é›¶ |
| const TYPE_ID | `T::TYPE_ID` æ˜¯ç¼–è¯‘æ—¶å¸¸é‡ï¼Œåˆ†æ”¯è¢«ä¼˜åŒ–æ¶ˆé™¤ | é›¶ |
| `#[inline(always)]` | å¼ºåˆ¶å†…è”ï¼Œæ— å‡½æ•°è°ƒç”¨å¼€é”€ | é›¶ |
| åŸå§‹åˆ‡ç‰‡ | `&[T]` æ— ä»»ä½•æŠ½è±¡å±‚ | é›¶ |

### æ³›å‹ Float Trait

```rust
pub trait Float: Copy + Send + Sync + 'static + Default {
    /// ç¼–è¯‘æ—¶ç±»å‹æ ‡è¯†ï¼Œç”¨äº GPU kernel é€‰æ‹©ï¼ˆconst = é›¶æˆæœ¬ï¼‰
    const TYPE_ID: FloatType;

    fn zero() -> Self;
    fn one() -> Self;
    fn from_f32(v: f32) -> Self;
    fn to_f32(self) -> f32;
    fn sqrt(self) -> Self;
    fn exp(self) -> Self;
    fn max(self, other: Self) -> Self;
}

impl Float for f32 { const TYPE_ID: FloatType = FloatType::F32; /* ... */ }
impl Float for f16 { const TYPE_ID: FloatType = FloatType::F16; /* ... */ }
impl Float for bf16 { const TYPE_ID: FloatType = FloatType::BF16; /* ... */ }
```

### ç»Ÿä¸€ç®—å­ç­¾å

```rust
// çº¯æ³›å‹ API - ç¼–è¯‘æ—¶å•æ€åŒ– = é›¶æˆæœ¬
pub fn flash_attention<T: Float>(
    q: &[T], k: &[T], v: &[T],
    output: &mut [T],
    batch: usize, seq_len: usize, num_heads: usize, head_dim: usize,
) -> Result<(), KernelError>;

pub fn softmax<T: Float>(
    input: &[T],
    output: &mut [T],
    shape: (usize, usize),
) -> Result<(), KernelError>;
```

### ç¦æ­¢çš„æ¨¡å¼

| ç¦æ­¢ | åŸå›  | æ­£ç¡®åšæ³• |
|------|------|----------|
| âŒ `flash_attention_f32()` | ç±»å‹åç¼€ = ä»£ç é‡å¤ | âœ… `flash_attention::<f32>()` |
| âŒ `Tensor<B, D>` | Burn æŠ½è±¡ = è¿è¡Œæ—¶å¼€é”€ | âœ… `&[T]` åŸå§‹åˆ‡ç‰‡ |
| âŒ è¿è¡Œæ—¶ç±»å‹åˆ¤æ–­ | `if type == f32` = åˆ†æ”¯å¼€é”€ | âœ… `T::TYPE_ID` const åˆ†æ”¯ |
| âŒ trait object | `dyn Float` = vtable å¼€é”€ | âœ… `<T: Float>` é™æ€åˆ†å‘ |

### GPU Kernel å®ç°ç»†èŠ‚ï¼ˆå†…éƒ¨ï¼Œç”¨æˆ·ä¸å¯è§ï¼‰

GPU kernel æ–‡ä»¶æŒ‰ç±»å‹åˆ†å¼€ï¼ˆPTX/HSACO ä¸æ”¯æŒæ³›å‹ï¼‰ï¼Œä½†è¿™æ˜¯**å®ç°ç»†èŠ‚**ï¼š

```rust
// å†…éƒ¨å®ç° - const åˆ†æ”¯è¢«ç¼–è¯‘å™¨å®Œå…¨ä¼˜åŒ–æ‰
#[inline(always)]
fn dispatch_kernel<T: Float>(...) {
    // T::TYPE_ID æ˜¯ constï¼Œç¼–è¯‘å™¨ç›´æ¥æ¶ˆé™¤å…¶ä»–åˆ†æ”¯
    match T::TYPE_ID {
        FloatType::F32 => /* ç›´æ¥å†…è” f32 kernel è°ƒç”¨ */,
        FloatType::F16 => /* ç›´æ¥å†…è” f16 kernel è°ƒç”¨ */,
        FloatType::BF16 => /* ç›´æ¥å†…è” bf16 kernel è°ƒç”¨ */,
    }
}
```

> **ç¼–è¯‘ç»“æœ**ï¼š`flash_attention::<f32>()` ç›´æ¥ç¼–è¯‘ä¸º f32 kernel è°ƒç”¨ï¼Œæ— ä»»ä½•åˆ†æ”¯åˆ¤æ–­ã€‚

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰å…¬å¼€ API ä½¿ç”¨ `<T: Float>` æ³›å‹
- [ ] æ— ä»»ä½• `_f32`ã€`_f16`ã€`_bf16` åç¼€çš„å…¬å¼€å‡½æ•°
- [ ] æ— ä»»ä½• `dyn` trait object
- [ ] æ— ä»»ä½•è¿è¡Œæ—¶ç±»å‹åˆ¤æ–­ï¼ˆä»… const åˆ†æ”¯ï¼‰
- [ ] Float trait æ”¯æŒ f32ã€f16ã€bf16 ä¸‰ç§ç±»å‹
- [ ] æ‰€æœ‰çƒ­è·¯å¾„å‡½æ•°æ ‡è®° `#[inline(always)]`

---

## ADR-001: åˆ é™¤ Burn ä¾èµ–ï¼Œç»Ÿä¸€åˆ° kernel_dispatcherï¼ˆ2025-01-16ï¼‰

### èƒŒæ™¯

gllm-kernels å­˜åœ¨ä¸¤å¥—ç‹¬ç«‹å®ç°ï¼š
- **ops/ å±‚**ï¼šBurn Tensor æŠ½è±¡ï¼ŒåŒ…å«è®ºæ–‡ä¼˜åŒ–ç®—æ³•
- **kernel_dispatcher**ï¼šåŸå§‹åˆ‡ç‰‡ `&[T]`ï¼ŒGPU è°ƒåº¦å±‚

### é—®é¢˜

| é—®é¢˜ | å½±å“ |
|------|------|
| Burn Tensor å¼€é”€ | å†…å­˜å¸ƒå±€ã€trait dispatchã€æ— æ³•å†…è” |
| GPU è°ƒåº¦åˆ†ç¦» | ops/ è®ºæ–‡ç®—æ³•æ— æ³•ä½¿ç”¨ GPU åŠ é€Ÿ |

### ğŸš¨ é‡è¦å‘ç°ï¼šops/ åŒ…å«è®ºæ–‡ä¼˜åŒ–ï¼Œä¸èƒ½åˆ é™¤

ops/ å±‚åŒ…å«å¤šè½®åŸºäºè®ºæ–‡çš„ç®—æ³•ä¼˜åŒ–å®ç°ï¼š

| ç®—æ³• | è®ºæ–‡æ¥æº | ops/ æ ¸å¿ƒä¼˜åŒ– | kernel_dispatcher ç°çŠ¶ |
|------|----------|--------------|----------------------|
| EAGLE-3 | NeurIPS'25 | å¤šå±‚èåˆ + è‡ªé€‚åº”è°ƒåº¦ï¼ˆ984è¡Œï¼‰ | ä»… GPU è°ƒåº¦å±‚ |
| Medusa | ICML'24 | N-gram ç¼“å­˜ + æ ‘ç”Ÿæˆï¼ˆ818è¡Œï¼‰ | ä»… GPU è°ƒåº¦å±‚ |
| FlashAttention | FlashAttention-2 | åˆ†å±‚å— + MaskCache LRUï¼ˆ1326è¡Œï¼‰ | ç®€åŒ– CPU fallback |
| Softmax | æ•°å€¼ç¨³å®šæ€§ | Log-space + Kahan ç´¯åŠ ï¼ˆ433è¡Œï¼‰ | åŸºç¡€å®ç° |
| PagedAttention | vLLM | å¤šçº§å±‚çº§ + CoW ç®¡ç† | åŸºç¡€å®ç° |

### å†³ç­–ï¼šè¿ç§» ops/ ç®—æ³•åˆ° kernel_dispatcherï¼ˆğŸš¨ é“å¾‹ï¼‰

**æ­£ç¡®åšæ³•**ï¼šè¿ç§»è®ºæ–‡ä¼˜åŒ–ç®—æ³•ï¼Œå»é™¤ Burn ä¾èµ–ï¼Œè€Œéåˆ é™¤

```
è¿ç§»ç­–ç•¥ï¼š
â”œâ”€â”€ é˜¶æ®µ1: å‡çº§ KernelFloat trait
â”‚   â”œâ”€â”€ æ·»åŠ  const TYPE_ID: FloatTypeï¼ˆé›¶æˆæœ¬åˆ†æ”¯æ¶ˆé™¤ï¼‰
â”‚   â””â”€â”€ æ·»åŠ  bf16 æ”¯æŒ
â”‚
â”œâ”€â”€ é˜¶æ®µ2: è¿ç§»è½»åº¦ä¾èµ–æ¨¡å—ï¼ˆBurn ä»…ä½œæ•°æ®å®¹å™¨ï¼‰
â”‚   â”œâ”€â”€ int2_quantizer.rs â†’ è½¬æ¢ Tensor<B,D> ä¸º &[T]
â”‚   â””â”€â”€ evic_press.rs     â†’ è½¬æ¢ Tensor<B,D> ä¸º &[T]
â”‚
â”œâ”€â”€ é˜¶æ®µ3: è¿ç§»ä¸­åº¦ä¾èµ–æ¨¡å—ï¼ˆä¿ç•™è®ºæ–‡æ ¸å¿ƒç®—æ³•ï¼‰
â”‚   â”œâ”€â”€ eagle3.rs   â†’ è¿ç§»å¤šå±‚èåˆ + è‡ªé€‚åº”è°ƒåº¦é€»è¾‘
â”‚   â”œâ”€â”€ spec_ee.rs  â†’ è¿ç§»æ¨æµ‹æ‰§è¡Œä¼˜åŒ–
â”‚   â””â”€â”€ medusa.rs   â†’ è¿ç§» N-gram ç¼“å­˜ + æ ‘ç”Ÿæˆ
â”‚
â”œâ”€â”€ é˜¶æ®µ4: è¿ç§»é‡åº¦ä¾èµ–æ¨¡å—ï¼ˆè®ºæ–‡æ ¸å¿ƒå®ç°ï¼‰
â”‚   â”œâ”€â”€ flash_attention.rs â†’ è¿ç§»åˆ†å±‚å— + MaskCache
â”‚   â”œâ”€â”€ softmax.rs         â†’ è¿ç§» Log-space + Kahan
â”‚   â””â”€â”€ paged_attention.rs â†’ è¿ç§»å¤šçº§å±‚çº§ + CoW
â”‚
â””â”€â”€ é˜¶æ®µ5: æ¸…ç†
    â””â”€â”€ åˆ é™¤ç©ºçš„ ops/ æ¨¡å—ï¼ˆè¿ç§»å®Œæˆåï¼‰

ä¿ç•™ï¼ˆå·²æ˜¯çº¯ Rustï¼Œæ— éœ€è¿ç§»ï¼‰ï¼š
â”œâ”€â”€ ops/engram*.rs              â†’ è¯­ä¹‰è®°å¿†ï¼Œçº¯ SIMD
â”œâ”€â”€ ops/embedding.rs            â†’ å‘é‡æ“ä½œï¼Œçº¯ SIMD
â””â”€â”€ ops/stable_accumulator.rs   â†’ æ•°å€¼å·¥å…·
```

### è¿ç§»æ¨¡å¼ï¼šBurn Tensor â†’ åŸå§‹åˆ‡ç‰‡

```rust
// è¿ç§»å‰ï¼ˆBurn Tensorï¼‰
pub fn flash_attention<B: Backend>(
    q: Tensor<B, 4>,
    k: Tensor<B, 4>,
    v: Tensor<B, 4>,
) -> Tensor<B, 4>

// è¿ç§»åï¼ˆé›¶æˆæœ¬æ³›å‹ï¼‰
pub fn flash_attention<T: Float>(
    q: &[T], k: &[T], v: &[T],
    output: &mut [T],
    batch: usize, seq_len: usize, num_heads: usize, head_dim: usize,
) -> Result<(), KernelError>
```

### ç»Ÿä¸€æ¶æ„

```
ç”¨æˆ· API
    â”‚
    â–¼
KernelDispatcherï¼ˆå”¯ä¸€å…¥å£ï¼‰
    â”‚
    â”œâ”€â”€ BackendType::Cuda  â†’ cuda_kernels/
    â”œâ”€â”€ BackendType::Rocm  â†’ hip_kernels/
    â”œâ”€â”€ BackendType::Metal â†’ metal_kernels/
    â”œâ”€â”€ BackendType::Wgpu  â†’ wgpu_kernels/
    â””â”€â”€ BackendType::Cpu   â†’ è®ºæ–‡ä¼˜åŒ–ç®—æ³•ï¼ˆä» ops/ è¿ç§»ï¼‰
```

### é›¶æˆæœ¬ä¿è¯

| è¦æ±‚ | å®ç° |
|------|------|
| æ—  Tensor æŠ½è±¡ | åŸå§‹åˆ‡ç‰‡ `&[T]` |
| æ—  vtable | enum + match æ´¾å‘ |
| å¼ºåˆ¶å†…è” | `#[inline(always)]` |
| ç¼–è¯‘æ—¶å•æ€åŒ– | æ³›å‹ `T: Float`ï¼ˆè§ ARCH-API-001ï¼‰ |
| const åˆ†æ”¯æ¶ˆé™¤ | `T::TYPE_ID` ç¼–è¯‘æ—¶ç¡®å®š |

### å…³è”çº¦æŸ

- **ARCH-API-001**ï¼šç»Ÿä¸€æ³›å‹ç®—å­ API è§„èŒƒï¼ˆğŸš¨ FROZEN - é›¶æˆæœ¬é“å¾‹ï¼‰

### çŠ¶æ€ï¼ˆ2025-01 æ›´æ–°ï¼‰

**å·²å®Œæˆ**ï¼š
- âœ… GPU kernel wrappersï¼ˆ4 å¹³å° Ã— 8 ç®—æ³•ï¼‰
- âœ… ARCH-API-001 é›¶æˆæœ¬é“å¾‹å®šä¹‰
- âœ… Burn ä¾èµ–å·²å®Œå…¨ç§»é™¤
- âœ… KernelFloat trait å·²å®ç°ï¼ˆconst TYPE_ID + bf16ï¼‰

**å½“å‰å®ç°**ï¼š
- ops/eagle3/ - EAGLE-3 æ¨¡å—ï¼ˆç›®å½•ç»“æ„ï¼‰
- ops/medusa/ - Medusa æ¨¡å—ï¼ˆç›®å½•ç»“æ„ï¼‰
- ops/spec_ee/ - SpecEE æ¨¡å—ï¼ˆç›®å½•ç»“æ„ï¼‰
- ops/softmax.rs - Log-space + Kahan å®ç°
- ops/paged_attn.rs - PagedAttention çº¯ Rust å®ç°
- ops/int2_quantizer.rs, ops/evic_press.rs - å·²è¿ç§»åˆ°çº¯ Rust

**å¾…å®Œæˆ**ï¼š
- ğŸš¨ flash_attention.rs - æœªå®ç°ï¼ˆå½“å‰é€šè¿‡ KernelDispatcher è°ƒç”¨ GPU kernelï¼‰
- ğŸš¨ éƒ¨åˆ†ç®—å­çš„ CPU fallback ä¼˜åŒ–

---

## é™„å½•ï¼šEngram æ¡ä»¶è®°å¿†æ”¯æŒ

DeepSeek Engram å®ç°è§„åˆ’ï¼Œè¯¦è§ `SPEC/DOCS/ENGRAM-DESIGN.md`ã€‚

| ç»„ä»¶ | å®ç°æ–¹å¼ | ä½ç½® |
|------|----------|------|
| æ ¸å¿ƒæ¨¡å— | æ¡ä»¶è®°å¿†ç®¡ç† | `ops/engram.rs` |
| N-gram å“ˆå¸Œ | SIMD ä¼˜åŒ–å“ˆå¸Œ | `ops/engram_hash.rs` |
| Embedding æŸ¥æ‰¾ | å†…å­˜æ˜ å°„ + prefetch | `ops/engram_lookup.rs` |

> âš ï¸ **å½“å‰çŠ¶æ€**ï¼šEngram ä½¿ç”¨çº¯ Rust + SIMD å®ç°ï¼ŒGPU kernel å°šæœªå®ç°ã€‚
